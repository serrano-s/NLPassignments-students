{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp5tokTea0QW"
      },
      "source": [
        "# Assignment 4: N-Gram Language Models (50 Points for both CSE 447 and CSE 517)\n",
        "\n",
        "Author: Kabir Ahuja\n",
        "\n",
        "Thanks to Melissa Mitchell and Khushi Khandelwal for feedback and Kavel Rao for designing the autograder.\n",
        "\n",
        "In this project, you will implement and experiment with N-gram language models. N-gram language models are simplest versions of a language model, which make a simplifying assumption that the probability of predicting a word in a sentence only depends on the past $N$ words in the sentence. In this project you will learn:\n",
        "\n",
        "- How to train word-level unigram and N-gram LMs on text data\n",
        "- Evaluating quality of an LM by computing perplexity\n",
        "- Sampling text from an N-gram LM\n",
        "- Implement Laplace Smoothing\n",
        "- Implement Interpolation for N-Gram Language Models\n",
        "\n",
        "We will be working with Shakespeare Plays from Andrej Karpathy's [blog post on Recurrent neural networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/). We also recommend going through chapter 3 of [Jurafsky and Martin](https://web.stanford.edu/~jurafsky/slp3/3.pdf) on N-Gram models, especially if you are not familiar with them yet.\n",
        "\n",
        "**Notes about the autograder for this assignment:**\n",
        "\n",
        "*   To submit the coding part of this assignment to Gradescope, download your .ipynb notebook in .ipynb form and submit **only** this file. It must be named **CSE447_Assignment4.ipynb** in order for the autograder to work.\n",
        "*   At a low-load time on Gradescope, the autograder for this assignment takes about **five minutes** to run. We recommend submitting the code part of your assignment with enough time before the deadline to allow the autograder to run, and for you to investigate/correct any issues it flags."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "# get any other necessary files for this project\n",
        "\n",
        "if [ ! -e \"data-needed.txt\" ]; then\n",
        "  if [ ! -e \"data_path_to_download_url.py\" ]; then\n",
        "    wget https://raw.githubusercontent.com/serrano-s/NLPassignments-students/refs/heads/main/data_path_to_download_url.py\n",
        "  else\n",
        "    echo \"data_path_to_download_url.py script already downloaded to runtime\"\n",
        "  fi\n",
        "\n",
        "  wget https://raw.githubusercontent.com/serrano-s/NLPassignments-students/refs/heads/main/assignments/NgramLanguageModels/data-needed.txt\n",
        "\n",
        "  # download all data files needed for the student-release version of this project (i.e., no hidden test files)\n",
        "  DATA_NEEDED_FILE=\"data-needed.txt\"\n",
        "  closing_slash=\"/\"\n",
        "  while IFS= read -r line; do\n",
        "    line=\"$(echo -e \"${line}\" | sed -e 's/^[[:space:]]*//' -e 's/[[:space:]]*$//')\";\n",
        "    dirs_to_check=\"${line%${closing_slash}*}\"\n",
        "    mkdir -p $dirs_to_check\n",
        "    download_url=$(python data_path_to_download_url.py \"$line\")\n",
        "    echo $download_url;\n",
        "    wget \"$download_url\" -O \"$line\"\n",
        "  done < \"$DATA_NEEDED_FILE\"\n",
        "else\n",
        "  echo \"data-needed.txt (and presumably therefore all necessary data files) already downloaded to runtime\"\n",
        "fi\n",
        "\n",
        "if [ ! -e \"other-setup-needed.sh\" ]; then\n",
        "  wget https://raw.githubusercontent.com/serrano-s/NLPassignments-students/refs/heads/main/assignments/NgramLanguageModels/other-setup-needed.sh\n",
        "  bash other-setup-needed.sh\n",
        "  rm data_path_to_download_url.py\n",
        "else\n",
        "  echo \"other-setup-needed.sh (and presumably therefore all other necessary files) already downloaded to runtime\"\n",
        "fi"
      ],
      "metadata": {
        "id": "aifT8FCKoh_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpXDtYj0a0QX"
      },
      "outputs": [],
      "source": [
        "# Load necessary packages\n",
        "import os\n",
        "from typing import List, Dict\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Setup nltk\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FACD7uBkbPs6"
      },
      "outputs": [],
      "source": [
        "# Set data directory. Important: DO NOT CHANGE THIS. AUTOGRADER WILL FAIL ON YOUR SUBMISSION OTHERWISE\n",
        "parent_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
        "data_dir = os.path.join(parent_dir, \"data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJ3c4n2Xa0QZ"
      },
      "outputs": [],
      "source": [
        "# Helper functions for sample test cases\n",
        "\n",
        "def evaluate_test_case(input, output, expected_output, output_str = \"Output\", atol=1e-4) -> Dict:\n",
        "\n",
        "    if input is not None:\n",
        "        print(\"Input:\\n\", input)\n",
        "    print(f\"{output_str}:\\n\", output)\n",
        "    print(f\"Expected {output_str}:\\n\", expected_output)\n",
        "\n",
        "    match = (\n",
        "        output == expected_output\n",
        "        if type(output) == str\n",
        "        else np.allclose(output, expected_output, atol=atol)\n",
        "    )\n",
        "\n",
        "    if match:\n",
        "        print(\"Test case passed! :)\")\n",
        "\n",
        "    else:\n",
        "        print(\"Test case failed! :(\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "\n",
        "def evaluate_list_test_case(input: List, output: List, expected_output: List) -> Dict:\n",
        "\n",
        "    print(\"Input:\\n\", input)\n",
        "    print(\"Output:\\n\", output)\n",
        "    print(\"Expected output:\\n\", expected_output)\n",
        "    if output == expected_output:\n",
        "        print(\"Test case passed! :)\")\n",
        "\n",
        "    else:\n",
        "        print(\"Test case failed! :(\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3h1pNcfea0QZ"
      },
      "source": [
        "## Part 1: Word-level unigram language models (11 points)\n",
        "\n",
        "We will start by considering word-level language models i.e. language models where the smallest unit (or a unigram) that can be predicted by the model is a word. In part 1, we will implement unigram language models, which constitutes the simplest variant of N-gram models -- simply learn the distribution of each unigram (here a word) in the corpus. Recall from the lectures, that for a text sequence with unigrams $w_1, w_2, \\cdots, w_n$, unigram language models, the probability of the sequence is given as:\n",
        "\n",
        "$$P(w_1, w_2, \\cdots, w_n) =P(w_1)P(w_2)\\cdots P(w_n)$$\n",
        "\n",
        "where $P(w_i)$ is simply the frequency of the word $w_i$ in the training corpus.\n",
        "\n",
        "For this part we will work with the Shakespeare dataset. Let's start by loading the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3zVNj_za0QZ"
      },
      "outputs": [],
      "source": [
        "with open(f\"{data_dir}/shakespeare/shakespeare_train.txt\") as f:\n",
        "    train_data = f.read().split(\"\\n\")\n",
        "\n",
        "with open(f\"{data_dir}/shakespeare/shakespeare_dev.txt\") as f:\n",
        "    dev_data = f.read().split(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biHeGfg-a0Qa"
      },
      "source": [
        "Below we print first 10 sentences from the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mf5_xvN6a0Qa"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\".join(train_data[:10]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnYwSzfZa0Qa"
      },
      "source": [
        "### Exercise 1.0 Text Processing\n",
        "\n",
        "Before start training our models, let's perform some basic preprocessing. For this exercise, we only want to put an \\<eos\\> tag at the end of each sentence in the training and dev sets. Implement `add_eos` function below that does that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcC3T-Sla0Qa",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-83e8a74c3faf2ec2",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def add_eos(data: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Adds an <eos> token to the end of each line in the data.\n",
        "\n",
        "    Inputs:\n",
        "    - data: a list of strings where each string is a line of text\n",
        "\n",
        "    Returns:\n",
        "    - a list of strings where each string is a line of text with <eos> token appended\n",
        "    \"\"\"\n",
        "\n",
        "    raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSmb99Qba0Qa"
      },
      "outputs": [],
      "source": [
        "def test_add_eos():\n",
        "    print(\"Running Sample Test Case 1\")\n",
        "    data = [\"hello!\", \"world!\"]\n",
        "\n",
        "    evaluate_list_test_case(data, add_eos(data), [\"hello! <eos>\", \"world! <eos>\"])\n",
        "\n",
        "    print(\"Running Sample Test Case 2\")\n",
        "    data = [\n",
        "        \"Many years later , as he faced the firing squad , Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice .\",\n",
        "        \"At that time Macondo was a village of twenty adobe houses, built on the bank of a river of clear water that ran along a bed of polished stones, which were white and enormous, like prehistoric eggs .\",\n",
        "        \"The world was so recent that many things lacked names, and in order to indicate them it was necessary to point .\" ]\n",
        "\n",
        "    expected_output = [\n",
        "        \"Many years later , as he faced the firing squad , Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice . <eos>\",\n",
        "        \"At that time Macondo was a village of twenty adobe houses, built on the bank of a river of clear water that ran along a bed of polished stones, which were white and enormous, like prehistoric eggs . <eos>\",\n",
        "        \"The world was so recent that many things lacked names, and in order to indicate them it was necessary to point . <eos>\"]\n",
        "    evaluate_list_test_case(data, add_eos(data), expected_output)\n",
        "\n",
        "test_add_eos()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9QedErya0Qb"
      },
      "outputs": [],
      "source": [
        "train_data_processed = add_eos(train_data)\n",
        "dev_data_processed = add_eos(dev_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8pqIK-Fa0Qb"
      },
      "source": [
        "### Exercise 1.1: Training (word-level) Unigram Language Model (2 Points)\n",
        "\n",
        "Training a unigram model simply corresponds to calculating frequencies of each word in the corpus, i.e.\n",
        "\n",
        "$$p(w_i) = \\frac{C(w_i)}{n}$$\n",
        "\n",
        "where $C(w_i)$ is the count of word $w_i$ in the training data and $n$ is the total number of words in the training dataset.\n",
        "\n",
        "Implement the `train_word_unigram` function below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gjpe86Fa0Qb",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-8ae19fe766eed890",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def train_word_unigram(train_data: List[str]) -> Dict[str, float]:\n",
        "\n",
        "    \"\"\"\n",
        "    Trains a word-level unigram language model.\n",
        "\n",
        "    Inputs:\n",
        "        - train_data: List[str], list of sentences in training data\n",
        "\n",
        "    Outputs:\n",
        "        - Dict[str, float], a dictionary mapping words to their unigram probabilities\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    unigram_probs = {}\n",
        "\n",
        "\n",
        "    raise NotImplementedError\n",
        "\n",
        "    return unigram_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKhUIFkza0Qc"
      },
      "outputs": [],
      "source": [
        "unigram_probs = train_word_unigram(train_data_processed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKDsM0zCa0Qc",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-f3e22e1c9dfc5a64",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Sample test cases\n",
        "def test_train_word_unigram(unigram_probs):\n",
        "\n",
        "    print(\"Running Sample Test Case 1: Check if the number of unique words is correct\")\n",
        "    evaluate_test_case(None, len(unigram_probs), 12610, output_str=\"Number of unique words\")\n",
        "\n",
        "    print(\"Running Sample Test Case 2: Check if the probability of word \\\"thou\\\" is correct\")\n",
        "    evaluate_test_case(\"thou\", unigram_probs[\"thou\"], 0.004559649641510829)\n",
        "\n",
        "    print(\"Running Sample Test Case 3: Check if the probability of word \\\"love\\\" is correct\")\n",
        "    evaluate_test_case(\"love\", unigram_probs[\"love\"], 0.0015984182127282134)\n",
        "\n",
        "    print(\"Running Sample Test Case 4: Check if the probability of word \\\"Richard\\\" is correct\")\n",
        "    evaluate_test_case(\"Richard\", unigram_probs[\"Richard\"], 0.0005035479340675586)\n",
        "\n",
        "    print(\"Running Sample Test Case 5: Check if the probability of word \\\"pytorch\\\" is correct\")\n",
        "    evaluate_test_case(\"pytorch\", unigram_probs.get(\"pytorch\", 0), 0)\n",
        "\n",
        "    print(\"Running Sample Test Case 6: Check if the probability of word \\\"richard\\\" is correct\")\n",
        "    evaluate_test_case(\"richard\", unigram_probs.get(\"richard\", 0), 0)\n",
        "\n",
        "\n",
        "test_train_word_unigram(unigram_probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94dSgcAHa0Qc"
      },
      "source": [
        "### Exercise 1.2: Evaluating (word-level) Unigram Language Models using Perplexity (2 Points)\n",
        "\n",
        "Now that we have trained our first (albeit very basic) language model, our next job is to evaluate how good of a job it does in modeling the training text as well as generalizing on the unseen text. The most commonly used metric for evaluating the quality of a language model is Perplexity. Recall from the lecture, perplexity of a language model on a test dataset measures the (inverse) probability assigned by the language model to the test dataset normalized by the number of words (or tokens). Lower the perplexity the higher probability the model assigns to the text in the test dataset and hence better quality.\n",
        "\n",
        "$$\\text{perplexity}(W) = P(w_1w_2\\cdots w_n)^{\\frac{-1}{n}} = \\sqrt[n]{\\frac{1}{P(w_1w_2\\cdots w_n)}}$$\n",
        "\n",
        "where $W$ is a test set with $n$ words $w_1w_2\\cdots w_n$\n",
        "\n",
        "It is useful to do perplexity calculation in log space to avoid numerical issues\n",
        "\n",
        "$$\\text{perplexity}(W) = \\exp\\bigl(-\\frac{\\log{P(w_1w_2\\cdots w_n)}}{n}\\bigr)$$\n",
        "\n",
        "When we have multiple sentences in the corpus and we assume sentences to be independent, we can write:\n",
        "\n",
        "$$\\text{perplexity}(W) = \\exp\\bigl(-\\frac{ \\sum_{S \\in W}  \\log{P(s_1s_2\\cdots s_{n_s})}}{n}\\bigr)$$\n",
        "\n",
        "where $S$ is a sentence in the corpus $W$ with words $s_1 s_2 \\cdots s_{n_s}$ and $n_s$ is the number of words in $S$.\n",
        "\n",
        "Note that assuming sentences to be independent is something that is not actually true in practice. However, since N-gram language models are already limited in their context, it is not the greatest loss to remove dependencies between sentences. In the future homeworks we will be dropping this assumption as we build more powerful models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8eUgzZ-a0Qc",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-669e5cc008545af9",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def eval_ppl_word_unigram(eval_data: List[str], unigram_probs: Dict[str, float]) -> float:\n",
        "\n",
        "    \"\"\"\n",
        "    Evaluates the perplexity of a word-level unigram language model on the dev set.\n",
        "\n",
        "    Inputs:\n",
        "        - dev_data: List[str], list of sentences in the evaluation data\n",
        "        - unigram_probs: Dict[str, float], a dictionary mapping words to their unigram probabilities\n",
        "\n",
        "    Outputs:\n",
        "        - float, the perplexity of the unigram language model on the evaluation set\n",
        "\n",
        "    Note 1: It is useful to do the calculations in log space and convert the final answer to original space to\n",
        "    avoid numerical issues .\n",
        "    Note 2: Assign 0 probability to words that are not in the unigram_probs dictionary, since those words were not seen during training.\n",
        "    \"\"\"\n",
        "\n",
        "    perplexity = 0\n",
        "\n",
        "\n",
        "    raise NotImplementedError\n",
        "\n",
        "    return perplexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F26q5YDJa0Qc"
      },
      "source": [
        "First let's compute the perplexity of training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eiS3E8ja0Qd"
      },
      "outputs": [],
      "source": [
        "train_ppl = eval_ppl_word_unigram(train_data_processed, unigram_probs)\n",
        "print(train_ppl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSfzzCHTa0Qd"
      },
      "source": [
        "You should expect a perplexity around 575 on the training data. Now let's evaluate on dev data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbJu9iLia0Qd"
      },
      "outputs": [],
      "source": [
        "dev_ppl = eval_ppl_word_unigram(dev_data_processed, unigram_probs)\n",
        "print(dev_ppl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ae7xVZza0Qd"
      },
      "source": [
        "You should see a RuntimeWarning: divide by zero and a perplexity of infinity in this case. Why did this happen? Because we have some words in the dev dataset, which were never seen during training. The unigram model assigns a zero probability to those, which result in an infinite perplexity (inverse probability)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HhEVEV0a0Qd"
      },
      "source": [
        "### Exercise 1.3: Handling Unknown Words (2 Points)\n",
        "\n",
        "How do we deal with such situations? An easy solution is to introduce an unknown word token, e.g. \\<unk\\>. Before training, we replace the words which occur less than a threshold number of times with an \\<unk\\> token. E.g., if a word occurs less than 3 times in the dataset, we replace it with the \\<unk\\> token. At test time, when evaluating the perplexity if we see a word, which was unseen during training i.e. has a unigram frequency of 0, we assign that word the probability of \\<unk\\> token.\n",
        "\n",
        "Implement the `replace_rare_words_with_unks` function below which replaces words which occur less than `unk_thresh` number of times by \\<unk\\> token. Also reimplement `eval_ppl_word_unigram` to handle unseen words. You might find the [`Counter` class from the `collections` module useful](https://docs.python.org/3/library/collections.html#counter-objects)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIZo6Dy0a0Qd",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-7335b83debed50b1",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def replace_rare_words_with_unks(train_data: List[str], unk_thresh: int) -> List[str]:\n",
        "    \"\"\"\n",
        "    Replaces words that occur less than unk_thresh times in the training data with \"<unk>\" token.\n",
        "\n",
        "    Inputs:\n",
        "        - train_data: List[str], list of sentences in the training data\n",
        "        - unk_thresh: int, the threshold on the number of occurrences of a word to be considered known (less than considered <unk>)\n",
        "\n",
        "    Outputs:\n",
        "        - List[str], the training text with rare words replaced by \"<unk>\" token\n",
        "    \"\"\"\n",
        "\n",
        "    train_data_unked = []\n",
        "\n",
        "\n",
        "    raise NotImplementedError\n",
        "\n",
        "    return train_data_unked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86wcwrw7a0Qe"
      },
      "outputs": [],
      "source": [
        "# Sample test cases\n",
        "def test_train_word_unigram_wth_unks():\n",
        "\n",
        "    print(\"# Testing for Threshold 3\\n\\n\")\n",
        "    train_data_wth_unks = replace_rare_words_with_unks(train_data_processed, 3)\n",
        "    unigram_probs_wth_unks = train_word_unigram(train_data_wth_unks)\n",
        "    print(\"Sample Test Case 1: Check if the number of unique words is correct\")\n",
        "    evaluate_test_case(None, len(unigram_probs_wth_unks), 4620, output_str=\"Number of unique words\")\n",
        "\n",
        "    print(\"Sample Test Case 2: Check if the probability of word \\\"<unk>\\\" is correct\")\n",
        "    evaluate_test_case(\"<unk>\", unigram_probs_wth_unks[\"<unk>\"], 0.045185342597383396)\n",
        "\n",
        "    print(\"# Testing for threshold 5\\n\\n\")\n",
        "    train_data_wth_unks = replace_rare_words_with_unks(train_data_processed, 5)\n",
        "    unigram_probs_wth_unks = train_word_unigram(train_data_wth_unks)\n",
        "    print(\"Sample Test Case 3: Check if the number of unique words is correct\")\n",
        "    evaluate_test_case(None, len(unigram_probs_wth_unks), 3088, output_str=\"Number of unique words\")\n",
        "\n",
        "    print(\"Sample Test Case 4: Check if the probability of word \\\"<unk>\\\" is correct\")\n",
        "    evaluate_test_case(\"<unk>\", unigram_probs_wth_unks[\"<unk>\"], 0.06910155961268387)\n",
        "\n",
        "test_train_word_unigram_wth_unks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohUNUEHfa0Qe"
      },
      "outputs": [],
      "source": [
        "# We will remove words that occur less than 3 times\n",
        "train_data_wth_unks = replace_rare_words_with_unks(train_data_processed, 3)\n",
        "unigram_probs_wth_unks = train_word_unigram(train_data_wth_unks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yykqyPYja0Qe"
      },
      "source": [
        "We recommend you to check the unigram probabilities before and after replacing rare words with \\<unk\\> token to verify your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhdUgHmza0Qe",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-7c1a766eb312e9ff",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def eval_ppl_word_unigram_with_unks(eval_data: List[str], unigram_probs_wth_unks: Dict[str, float]) -> float:\n",
        "\n",
        "    \"\"\"\n",
        "    Evaluates the perplexity of a word-level unigram language model on the dev set. For unseen words, uses the <unk> token probability.\n",
        "\n",
        "    Inputs:\n",
        "        - eval_data: string, List of sentences in eval data\n",
        "        - unigram_probs_wth_unks: Dict[str, float], a dictionary mapping words to their unigram probabilities, including <unk> token\n",
        "\n",
        "    Outputs:\n",
        "        - float, the perplexity of the unigram language model on the evaluation set\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    perplexity = 0\n",
        "\n",
        "\n",
        "    raise NotImplementedError\n",
        "\n",
        "    return perplexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHp-_Jlga0Qe"
      },
      "source": [
        "Let's compute the train and dev perplexity now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iggwsRkaa0Qe"
      },
      "outputs": [],
      "source": [
        "train_ppl = eval_ppl_word_unigram_with_unks(train_data_wth_unks, unigram_probs_wth_unks)\n",
        "print(f\"Train Perplexity: {train_ppl}\")\n",
        "dev_ppl = eval_ppl_word_unigram_with_unks(dev_data_processed, unigram_probs_wth_unks)\n",
        "print(f\"Dev Perplexity: {dev_ppl}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqdsNFOWa0Qe"
      },
      "source": [
        "You should observe a train perplexity around 384 and dev perplexity around 282."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9tIZTFha0Qf"
      },
      "source": [
        "### Exercise 1.4: Sampling from Unigram Language Model (2 Points)\n",
        "\n",
        "Now that we have trained and evaluated our unigram LM, we are ready to generate some text from it. To sample text from an N-gram language model given prefix words $w_1, w_2, \\cdots, w_n$, we sequentially sample next tokens from the N-gram probability distribution given the previous words, i.e.,\n",
        "\n",
        "$$w_{n+1} \\sim P(w_{n+1} | w_{1}, \\cdots, w_{n} )$$\n",
        "\n",
        "For a unigram language model, since $P(w_1, \\dots, w_n) = P(w_1)\\cdots P(w_n)$ i.e. all words all distributed independently and the next token is sampled independent of previous tokens, the above equation simplifies to:\n",
        "\n",
        "$$w_{n+1} \\sim P(w_{n+1})$$\n",
        "\n",
        "You will now implement the function `sample_from_word_unigram` below. You might find the [Numpy's random module (np.random)](https://numpy.org/doc/stable/reference/random/index.html) useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "at0epuaga0Qf",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-08f4a7dd328533c1",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def sample_from_word_unigram(unigram_probs: Dict[str, float], max_words: int, prefix: str = \"\") -> str:\n",
        "\n",
        "    \"\"\"\n",
        "    Samples sequence of words from a unigram language model.\n",
        "    Terminate sampling when either max_words is reached or when <eos> token is sampled.\n",
        "\n",
        "    Inputs:\n",
        "        - unigram_probs: Dict[str, float], a dictionary mapping words to their unigram probabilities\n",
        "        - n_words: int, the number of words to sample\n",
        "        - prefix: str, a prefix to start the sampling from. Can have multiple words separated by spaces.\n",
        "\n",
        "    Outputs:\n",
        "        - str: sampled text i.e. string of sampled words separated by spaces along with the prefix\n",
        "\n",
        "    # Note: Please use np.random.choice to sample from the unigram_probs dictionary.\n",
        "    \"\"\"\n",
        "    sampled_string = \"\"\n",
        "\n",
        "    raise NotImplementedError\n",
        "\n",
        "    return sampled_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVqzBMYsa0Qf"
      },
      "outputs": [],
      "source": [
        "# Sample test cases\n",
        "def test_sample_from_word_unigram():\n",
        "\n",
        "    np.random.seed(0)\n",
        "    sampled_string = sample_from_word_unigram(unigram_probs_wth_unks, 10, \"The king\")\n",
        "    print(\"Running Test Case 1: Check if the sampled string starts with the prefix\")\n",
        "    evaluate_test_case(None, sampled_string.startswith(\"The king\"), True, output_str=\"Sampled string starts with the prefix\")\n",
        "\n",
        "    print(\"Running Test Case 2: Check if the sampled string has either 10 generated words or ends with <eos> token\")\n",
        "    print(f\"Generated string: {sampled_string}\")\n",
        "    print(f\"Number of generated words: {len(sampled_string.split()) - 2}\")\n",
        "    print(f\"Does the generated string end with <eos> token: {'<eos>' in sampled_string}\")\n",
        "    if len(sampled_string.split()) - 2 == 10 or (\n",
        "        len(sampled_string.split()) - 2 < 10 and \"<eos>\" in sampled_string\n",
        "    ):\n",
        "        print(\"Test passed! :)\")\n",
        "    else:\n",
        "        print(\"Test failed! :(\")\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    print(\"Running Test Case 3: Check if the probability of generating <unk> token is correct\")\n",
        "\n",
        "    sampled_strings = [\n",
        "        sample_from_word_unigram(unigram_probs_wth_unks, 1, \"\")\n",
        "        for _ in range(1000)\n",
        "    ]\n",
        "    sampled_string = \" \".join(sampled_strings)\n",
        "    num_unks = sampled_string.count(\"<unk>\")\n",
        "    unk_gen_prob = num_unks / len(sampled_string.split())\n",
        "    evaluate_test_case(None, unk_gen_prob, unigram_probs_wth_unks[\"<unk>\"], output_str=\"Probability of generating <unk> token\", atol=1e-2)\n",
        "\n",
        "test_sample_from_word_unigram()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-_c4x__a0Qf"
      },
      "source": [
        "Note that due to the randomness in sampling, it is difficult to automatically test this function. However, you can use the technique we use in sample test case 3 by repeatedly sampling from the unigram model and checking the frequency of different words in the generated text and checking if they are close to the unigram probabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6dbTa5la0Qf"
      },
      "source": [
        "Let's sample some text from the unigram model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_X2FnG7a0Qf"
      },
      "outputs": [],
      "source": [
        "for _ in range(10):\n",
        "    sampled_string = sample_from_word_unigram(unigram_probs_wth_unks, 20)\n",
        "    print(sampled_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kyhg1D0ha0Qg"
      },
      "source": [
        "You should see that the generated that doesn't make a whole lot of sense, which is natural since we are using a unigram model that doesn't account for the context at all, and essentially samples the most common tokens in its generations. We will now move to build language models that do not have this problem, i.e., they take into account the context (albeit to different degrees) for modeling the distribution of words (or tokens) in the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtWblv6Za0Qg"
      },
      "source": [
        "### Write-Up Question 1: Effect of \\<unk\\> tokens on perplexity and generation quality (2 Points)\n",
        "\n",
        "What effect do you think the \\<unk\\> tokens will have on the perplexity of the model? Try out different thresholds for replacing rare words with \\<unk\\> token and report the perplexity on the training and dev set. What do you observe? Does the perplexity decrease with increasing threshold? Do you think that improves generation quality? (Your answer to this question should go in your separate write-up PDF.)\n",
        "\n",
        "**What to submit:**\n",
        "\n",
        "- A Table with the perplexity of the unigram model on the training and dev set for different thresholds of replacing rare words with \\<unk\\> token. You can choose the thresholds as 1, 3, 5,  7, 9, 10.\n",
        "\n",
        "- Example generations at each threshold.\n",
        "\n",
        "- 3-4 lines maximum discussing the trends you observe in the perplexity and generation quality with increasing threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-J9I62Vma0Qg"
      },
      "outputs": [],
      "source": [
        "\n",
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7rcesEQa0Qg"
      },
      "source": [
        "### Write-Up Question 2: Alterative to Random Sampling (1 Point)\n",
        "\n",
        "An alternate algorithm to generate text from a language model is greedy decoding, i.e., where we generate the most likely token at each step of decoding, i.e.\n",
        "\n",
        "$$ w_{k+1} = \\texttt{argmax}_{w} P(w | w_{1}, \\cdots, w_{k}) $$\n",
        "\n",
        "Can you explain why or why not that will be a good idea for unigram language models? Explain in no more in 3 lines. (Your answer to this question should go in your separate write-up PDF.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KckdzhrIa0Qg"
      },
      "source": [
        "## Part 2: N(>1)-Gram Word-Level Language Models (12 Points)\n",
        "\n",
        "We will now implement much more sophisticated language models, which make use of the surrounding text to model the distribution of text. Recall from the lectures for an $N$-gram language model with $n > 1$, the distribution of a sequence of tokens $w_1, w_2, \\cdots, w_n$ is given as:\n",
        "\n",
        "$$P(w_1, w_2, \\cdots, w_n) = \\prod_{k=1}^{n}P(w_k \\mid w_{k-N-1}, \\cdots, w_{k-1})$$\n",
        "\n",
        "E.g., for a bigram model i.e. $N = 2$, the expression becomes:\n",
        "\n",
        "$$P(w_1, w_2, \\cdots, w_n) = \\prod_{k=1}^{n}P(w_k \\mid w_{k-1})$$\n",
        "\n",
        "i.e. the distribution of a token depends solely on the tokens preceding it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etpnARwia0Qh"
      },
      "source": [
        "### Write-Up Question 3 (2 Points)\n",
        "\n",
        "Can you show why for an N-gram language model the following expression holds?\n",
        "\n",
        "$$P(w_1, w_2, \\cdots, w_n) = \\prod_{k=1}^{n}P(w_k \\mid w_{k-N-1}, \\cdots, w_{k-1})$$\n",
        "\n",
        "Lay down the assumption that is required to derive this expression and show all the steps in your derivation. (Your answer to this question should go in your separate write-up PDF.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHnlidO9a0Qh"
      },
      "outputs": [],
      "source": [
        "# Before we start, lets set training data to be the one with <unk> tokens and dev data to be the one with <eos> tokens in the end\n",
        "\n",
        "train_data = train_data_wth_unks\n",
        "dev_data = dev_data_processed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz3U2fIKa0Qh"
      },
      "source": [
        "### Exercise 2.1: Text Processing\n",
        "\n",
        "A careful reader must have noted that the above expression: $P(w_1, w_2, \\cdots, w_n) = \\prod_{k=1}^{n}P(w_k \\mid w_{k-1})$ has a problem. On the right hand side, when $k = 1$, this will result in the term $P(w_1 | w_0)$, but there is no $w_0$ in the sequence. Similarly, for a trigram language model we will have terms, $P(w_1 | w_{-1}, w_{0})$ and $P(w_2 | w_0, w_1)$ with conditionals on words that are not part of the sequence. To handle this issue, we add $N-1$ start of sequence tokens, e.g. \\<sos\\> to the beginning of the sequence.\n",
        "\n",
        "Implement `process_text_for_Ngram` function below that adds $N-1$ \\<sos\\> tokens to beginning of each sentence in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUj8S9qJa0Qh",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-2228a409c358fa6e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def process_text_for_Ngram(sents: List[str], N: int = 2) -> List[str]:\n",
        "\n",
        "    \"\"\"\n",
        "    Adds N-1 <sos> tokens to the start of every sentence in the text.\n",
        "\n",
        "    Inputs:\n",
        "        - sents: List[str], List of sentences\n",
        "        - N: int, the N in N-gram\n",
        "\n",
        "    Outputs:\n",
        "        - List[str], the processed text\n",
        "    \"\"\"\n",
        "    processed_sents = None\n",
        "\n",
        "\n",
        "    raise NotImplementedError\n",
        "\n",
        "    return processed_sents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJXmUNlka0Qh"
      },
      "outputs": [],
      "source": [
        "# Sample test cases\n",
        "\n",
        "def test_process_text_for_Ngram():\n",
        "\n",
        "    sents = [\"Many years later , as he faced the firing squad , Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice .\",\n",
        "    \"At that time Macondo was a village of twenty adobe houses, built on the bank of a river of clear water that ran along a bed of polished stones, which were white and enormous, like prehistoric eggs .\",\n",
        "    \"The world was so recent that many things lacked names, and in order to indicate them it was necessary to point .\" ]\n",
        "\n",
        "    print(\"Running Sample Test Case 1 with N=1\")\n",
        "    unigram_processed_text = process_text_for_Ngram(sents, 1)\n",
        "    excepted_output = [\n",
        "        \"Many years later , as he faced the firing squad , Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice .\",\n",
        "        \"At that time Macondo was a village of twenty adobe houses, built on the bank of a river of clear water that ran along a bed of polished stones, which were white and enormous, like prehistoric eggs .\",\n",
        "        \"The world was so recent that many things lacked names, and in order to indicate them it was necessary to point .\"\n",
        "    ]\n",
        "    evaluate_list_test_case(sents, unigram_processed_text, excepted_output)\n",
        "\n",
        "    print(\"Running Sample Test Case 2 with N=2\")\n",
        "    bigram_processed_text = process_text_for_Ngram(sents, 2)\n",
        "    excepted_output = [\n",
        "        \"<sos> Many years later , as he faced the firing squad , Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice .\",\n",
        "        \"<sos> At that time Macondo was a village of twenty adobe houses, built on the bank of a river of clear water that ran along a bed of polished stones, which were white and enormous, like prehistoric eggs .\",\n",
        "        \"<sos> The world was so recent that many things lacked names, and in order to indicate them it was necessary to point .\"\n",
        "    ]\n",
        "    evaluate_list_test_case(sents, bigram_processed_text, excepted_output)\n",
        "\n",
        "    print(\"Running Sample Test Case 3 with N=3\")\n",
        "    trigram_processed_text = process_text_for_Ngram(sents, 3)\n",
        "    excepted_output = [\n",
        "        \"<sos> <sos> Many years later , as he faced the firing squad , Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice .\",\n",
        "        \"<sos> <sos> At that time Macondo was a village of twenty adobe houses, built on the bank of a river of clear water that ran along a bed of polished stones, which were white and enormous, like prehistoric eggs .\",\n",
        "        \"<sos> <sos> The world was so recent that many things lacked names, and in order to indicate them it was necessary to point .\"\n",
        "    ]\n",
        "    evaluate_list_test_case(sents, trigram_processed_text, excepted_output)\n",
        "\n",
        "    print(\"Running Sample Test Case 4 with N=4\")\n",
        "    fourgram_processed_text = process_text_for_Ngram(sents, 4)\n",
        "    excepted_output = [\n",
        "        \"<sos> <sos> <sos> Many years later , as he faced the firing squad , Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice .\",\n",
        "        \"<sos> <sos> <sos> At that time Macondo was a village of twenty adobe houses, built on the bank of a river of clear water that ran along a bed of polished stones, which were white and enormous, like prehistoric eggs .\",\n",
        "        \"<sos> <sos> <sos> The world was so recent that many things lacked names, and in order to indicate them it was necessary to point .\"\n",
        "    ]\n",
        "    evaluate_list_test_case(sents, fourgram_processed_text, excepted_output)\n",
        "\n",
        "test_process_text_for_Ngram()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6xdEYhPa0Qi"
      },
      "source": [
        "### Exercise 2.2: Implementing N-gram language models (10 Points)\n",
        "\n",
        "The heart of implementing an N-gram language model is to estimate the conditional distributions $P(w_n \\mid w_{n-N-1}, \\cdots, w_{n-1})$. Recall from the lectures that the conditional distributions can be estimated as:\n",
        "\n",
        "$$P(w_n \\mid w_{n-N-1}, \\cdots, w_{n-1}) = \\frac{C(w_{n-N-1} \\cdots w_{n-1} w_{n})}{\\sum_{w \\in \\mathcal{W}}{C(w_{n-N-1} \\cdots w_{n-1} w)}} = \\frac{C(w_{n-N-1} \\cdots w_{n-1} w_{n})}{{C(w_{n-N-1} \\cdots w_{n-1})}}$$\n",
        "\n",
        "where $C(w_{n-N-1} \\cdots w_{n-1} w)$ is the number of times the token sequence $w_{n-N-1} \\cdots w_{n-1} w$ appears in the corpus, and $\\mathcal{W}$ is the vocabulary of the N-gram model.\n",
        "\n",
        "You will now implement an N-gram language model from scratch. Note that a full implementation involves a `fit` function, which computes the N-gram counts $C(w_{n-N-1} \\cdots w_{n-1} w)$ needed to compute the conditional distributions; `eval_perplexity` function, which evaluates the perplexity of the language model on a test set; and a `sample_text` function which generates the text from the LM. Implement the class `WordNGramLM` below with these functions.This time we leave all the implementation details for you to decide. We just provide a boiler plate code, with the expected input output for the three functions in the class. You might need to implement additional functions for your implementation of the three functions.\n",
        "\n",
        "**Note 1**: If implementing a general NGram Model from scratch seems too daunting, we recommend starting implementing just the Bigram LM and checking if you are able to pass BigramLM Test cases. From there you can work on generalizing your solution.\n",
        "\n",
        "**Note 2**: Efficiency of your code will be important here, especially for `eval_perplexity` and `sample_text` functions. A naive implementation of these functions will result in a time complexity of $\\mathcal{O}(nV)$ for `eval_perplexity` and $\\mathcal{O}(TV)$ for `sample_text`, where $n$ is the number of words in evaluation dataset, T is the length of sampled text, and $V$ is the size of vocabulary. By caching certain quantities during training, you should be able to write implementations with time complexities $\\mathcal{O}(n + V)$ and $\\mathcal{O}(T + V)$ for the two functions. We will give only half credit for the naive implementation.\n",
        "\n",
        "**Note 3**: We will provide sample test cases only to test `eval_perplexity` and `sample_text` functions and not for the `fit` function. Both of the former functions rely on the latter to be implemented correctly, hence use correctness of `eval_perplexity` and `sample_text` to check the correctness of your `fit` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7YHarZha0Qi",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-cf47ebd6c0f86836",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class WordNGramLM:\n",
        "\n",
        "    def __init__(self, N: int):\n",
        "        self.N = N\n",
        "\n",
        "    def fit(self, train_data: List[str]):\n",
        "\n",
        "        \"\"\"\n",
        "        Trains an N-gram language model.\n",
        "\n",
        "        Inputs:\n",
        "            - train_data: str, sentences in the training data\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def eval_perplexity(self, eval_data: List[str]) -> float:\n",
        "\n",
        "        \"\"\"\n",
        "        Evaluates the perplexity of the N-gram language model on the eval set.\n",
        "\n",
        "        Input:\n",
        "            - eval_data: List[str], the evaluation text\n",
        "\n",
        "        Output:\n",
        "            - float, the perplexity of the model on the evaluation set\n",
        "\n",
        "        Note : For words that are not in the vocabulary, replace them with the <unk> token.\n",
        "        Note : Don't count the <sos> tokens in your number of total tokens in order to match expected perplexities.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def sample_text(self, prefix: str = \"<sos>\", max_words: int = 100) -> float:\n",
        "\n",
        "        \"\"\"\n",
        "        Samples text from the N-gram language model.\n",
        "        Terminate sampling when either max_words is reached or when <eos> token is sampled.\n",
        "        Inputs:\n",
        "            - prefix: str, the prefix to start the sampling from. Can also be multiple words separated by spaces.\n",
        "            - max_words: int, the maximum number of words to sample\n",
        "\n",
        "        Outputs:\n",
        "            - str, the sampled text\n",
        "\n",
        "        Note: Please use np.random.choice for sampling next words\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # Extra utility functions that you think will be useful can go below\n",
        "\n",
        "    # YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-iy528mm990"
      },
      "outputs": [],
      "source": [
        "# <NO_AUTOGRADE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGuUcQRqa0Qi"
      },
      "source": [
        "As a sanity check, check if your code returns same perplexities when N=1 as your earlier unigram model perplexities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5nuPdtVa0Qi"
      },
      "outputs": [],
      "source": [
        "unigram_lm = WordNGramLM(1)\n",
        "unigram_lm.fit(train_data)\n",
        "\n",
        "train_ppl = unigram_lm.eval_perplexity(train_data)\n",
        "dev_ppl = unigram_lm.eval_perplexity(dev_data)\n",
        "print(f\"Train Perplexity for Unigram model (Class Implementation): {train_ppl}\")\n",
        "print(f\"Dev Perplexity for Unigram model (Class Implementation): {dev_ppl}\")\n",
        "\n",
        "train_ppl_old = eval_ppl_word_unigram_with_unks(train_data_wth_unks, unigram_probs_wth_unks)\n",
        "dev_ppl_old = eval_ppl_word_unigram_with_unks(dev_data_processed, unigram_probs_wth_unks)\n",
        "\n",
        "print(f\"Train Perplexity for Unigram model (Original Implementation): {train_ppl_old}\")\n",
        "print(f\"Dev Perplexity for Unigram model (Original Implementation): {dev_ppl_old}\")\n",
        "\n",
        "if np.allclose(train_ppl, train_ppl_old, atol = 1e-4) and np.allclose(dev_ppl, dev_ppl_old, atol = 1e-4):\n",
        "    print(\"Unigram model perplexities match! :)\")\n",
        "\n",
        "else:\n",
        "    print(\"Unigram model perplexities do not match! :(\")\n",
        "\n",
        "# assert np.allclose(train_ppl, train_ppl_old, atol = 1e-4)\n",
        "# assert np.allclose(dev_ppl, dev_ppl_old, atol = 1e-4)\n",
        "# print(\"Unigram model perplexities match!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhpeS8iKa0Qi"
      },
      "source": [
        "Test implementation of `eval_perplexity` for bigram model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzAV8eaxa0Qj"
      },
      "outputs": [],
      "source": [
        "# Test implementation of `eval_perplexity` for bigram model\n",
        "bigram_lm = WordNGramLM(2)\n",
        "bigram_lm.fit(train_data)\n",
        "train_ppl = bigram_lm.eval_perplexity(train_data)\n",
        "print(f\"Train Perplexity: {train_ppl}\")\n",
        "dev_ppl = bigram_lm.eval_perplexity(dev_data)\n",
        "print(f\"Dev Perplexity: {dev_ppl}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcYXlZwba0Qj"
      },
      "source": [
        "You should see a train perplexity of roughly 42 and dev perplexity infinite, we will soon see how to deal with that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNM5CenUa0Qj"
      },
      "source": [
        "Test implementation of `sample_text` for bigram model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nW87Z9h0a0Qk"
      },
      "outputs": [],
      "source": [
        "def test_sample_text_ngram():\n",
        "\n",
        "    print(\"Testing for Trigram model\")\n",
        "\n",
        "    random.seed(42)\n",
        "    np.random.seed(42)\n",
        "    trigram_lm = WordNGramLM(3)\n",
        "    trigram_lm.fit(train_data)\n",
        "    sampled_text = trigram_lm.sample_text(\"<sos> <sos>\", max_words=50)\n",
        "\n",
        "    print(\"Test Case 1: Check if the sampled text starts with <sos> <sos>\")\n",
        "    evaluate_test_case(None, sampled_text.startswith(\"<sos> <sos>\"), True, output_str=\"Sampled text starts with <sos> <sos>\")\n",
        "\n",
        "    print(\"Test Case 2: Check if the number of generated words is either 50 or less than 50 and ends with <eos>\")\n",
        "    print(f\"Generated text: {sampled_text}\")\n",
        "    print(f\"Number of generated words: {len(sampled_text.split()) - 2}\")\n",
        "    print(f\"Does the generated text end with <eos>: {'<eos>' in sampled_text}\")\n",
        "    if len(sampled_text.split()) - 2 == 50 or (\n",
        "        len(sampled_text.split()) - 2 < 50 and \"<eos>\" in sampled_text\n",
        "    ):\n",
        "        print(\"Test passed! :)\")\n",
        "    else:\n",
        "        print(\"Test failed! :(\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    print(\"Test Case 3: Check if the probability of generating II is greater than III when prefix is KING RICHARD\")\n",
        "    sampled_texts = [\n",
        "        trigram_lm.sample_text(\"KING RICHARD\", max_words=1) for _ in range(10000)\n",
        "    ]\n",
        "    sampled_text = \" \".join(sampled_texts)\n",
        "    num_richard_2s = [\n",
        "        text.split(\"KING RICHARD\")[1].strip() == \"II\" for text in sampled_texts\n",
        "    ].count(True)\n",
        "    num_richard_3s = [\n",
        "        text.split(\"KING RICHARD\")[1].strip() == \"III\" for text in sampled_texts\n",
        "    ].count(True)\n",
        "\n",
        "    gen_prob_richard_2 = num_richard_2s / len(sampled_texts)\n",
        "    gen_prob_richard_3 = num_richard_3s / len(sampled_texts)\n",
        "\n",
        "    print(f\"Probability of generating Richard II: {gen_prob_richard_2}\")\n",
        "    print(f\"Probability of generating Richard III: {gen_prob_richard_3}\")\n",
        "    if gen_prob_richard_2 < gen_prob_richard_3:\n",
        "        print(\"Test passed! :)\")\n",
        "    else:\n",
        "        print(\"Test failed! :(\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    print(\"Test Case 4: Check if the probability of generating II given KING RICHARD are close to the expected values\")\n",
        "    evaluate_test_case(\"King Richard II\", gen_prob_richard_2, 0.4051, output_str=\"Probability of generating Richard II\", atol=1e-2)\n",
        "\n",
        "    print(\"Test Case 5: Check if the probability of generating III given KING RICHARD are close to the expected values\")\n",
        "    evaluate_test_case(\"King Richard III\", gen_prob_richard_3, 0.5949, output_str=\"Probability of generating Richard III\", atol=1e-2)\n",
        "\n",
        "    print(\"Testing for 4-gram model\")\n",
        "    random.seed(42)\n",
        "    np.random.seed(42)\n",
        "    fourgram_lm = WordNGramLM(4)\n",
        "    fourgram_lm.fit(train_data)\n",
        "    # sampled_text = fourgram_lm.sample_text(\"<sos> <sos>\", max_words=50)\n",
        "\n",
        "    sampled_text = fourgram_lm.sample_text(\"<sos> <sos> <sos>\", max_words=50)\n",
        "\n",
        "    print(\"Test Case 6: Check if the sampled text starts with <sos> <sos> <sos>\")\n",
        "    evaluate_test_case(None, sampled_text.startswith(\"<sos> <sos> <sos>\"), True, output_str=\"Sampled text starts with <sos> <sos> <sos>\")\n",
        "\n",
        "    print(\"Test Case 7: Check if the number of generated words is either 50 or less than 50 and ends with <eos>\")\n",
        "    print(f\"Generated text: {sampled_text}\")\n",
        "    print(f\"Number of generated words: {len(sampled_text.split()) - 3}\")\n",
        "    print(f\"Does the generated text end with <eos>: {'<eos>' in sampled_text}\")\n",
        "    if len(sampled_text.split()) - 3 == 50 or (\n",
        "        len(sampled_text.split()) - 3 < 50 and \"<eos>\" in sampled_text\n",
        "    ):\n",
        "        print(\"Test passed! :)\")\n",
        "\n",
        "    else:\n",
        "        print(\"Test failed! :(\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    print(\"Test Case 8: Check if the probability of generating II is greater than III when prefix is <sos> KING RICHARD\")\n",
        "    sampled_texts = [\n",
        "        fourgram_lm.sample_text(\"<sos> <sos> KING RICHARD\", max_words=1) for _ in range(10000)\n",
        "    ]\n",
        "    sampled_text = \" \".join(sampled_texts)\n",
        "    num_richard_2s = [\n",
        "        text.split(\"<sos> <sos> KING RICHARD\")[1].strip() == \"II\" for text in sampled_texts\n",
        "    ].count(True)\n",
        "\n",
        "    num_richard_3s = [\n",
        "        text.split(\"<sos> <sos> KING RICHARD\")[1].strip() == \"III\" for text in sampled_texts\n",
        "    ].count(True)\n",
        "\n",
        "    gen_prob_rich2 = num_richard_2s / len(sampled_texts)\n",
        "    gen_prob_rich3 = num_richard_3s / len(sampled_texts)\n",
        "\n",
        "    print(f\"Probability of generating Richard II: {gen_prob_rich2}\")\n",
        "    print(f\"Probability of generating Richard III: {gen_prob_rich3}\")\n",
        "    if gen_prob_rich2 < gen_prob_rich3:\n",
        "        print(\"Test passed! :)\")\n",
        "    else:\n",
        "        print(\"Test failed! :(\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    print(\"Test Case 9: Check if the probability of generating II given <sos> KING RICHARD are close to the expected values\")\n",
        "    evaluate_test_case(\"<sos> King Richard II\", gen_prob_rich2, 0.4044, output_str=\"Probability of generating Richard II\", atol=1e-2)\n",
        "\n",
        "    print(\"Test Case 10: Check if the probability of generating III given <sos> KING RICHARD are close to the expected values\")\n",
        "    evaluate_test_case(\"<sos> King Richard III\", gen_prob_rich3, 0.5956, output_str=\"Probability of generating Richard III\", atol=1e-2)\n",
        "\n",
        "test_sample_text_ngram()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEFfpoDza0Qj"
      },
      "outputs": [],
      "source": [
        "def test_sample_text_bigram_model():\n",
        "    bigram_lm = WordNGramLM(2)\n",
        "    bigram_lm.fit(train_data)\n",
        "\n",
        "    random.seed(42)\n",
        "    np.random.seed(42)\n",
        "    sampled_text = bigram_lm.sample_text(\"<sos>\", max_words=50)\n",
        "\n",
        "    print(\"Test Case 1: Check if the sampled text starts with <sos>\")\n",
        "    evaluate_test_case(None, sampled_text.startswith(\"<sos>\"), True, output_str=\"Sampled text starts with <sos>\")\n",
        "\n",
        "    print(\"Test Case 2: Check if the number of generated words is either 50 or less than 50 and ends with <eos>\")\n",
        "    print(f\"Generated text: {sampled_text}\")\n",
        "    print(f\"Number of generated words: {len(sampled_text.split()) - 1}\")\n",
        "    print(f\"Does the generated text end with <eos>: {'<eos>' in sampled_text}\")\n",
        "    if len(sampled_text.split()) - 1 == 50 or (\n",
        "        len(sampled_text.split()) < 50 and \"<eos>\" in sampled_text\n",
        "    ):\n",
        "        print(\"Test passed! :)\")\n",
        "    else:\n",
        "        print(\"Test failed! :(\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    print(\"Test Case 3: Check if the probability of generating II is greater than III when prefix is RICHARD\")\n",
        "    sampled_texts = [\n",
        "        bigram_lm.sample_text(\"RICHARD\", max_words=1) for _ in range(10000)\n",
        "    ]\n",
        "    sampled_text = \" \".join(sampled_texts)\n",
        "    num_richard_2s = [\n",
        "        text.split(\"RICHARD\")[1].strip() == \"II\" for text in sampled_texts\n",
        "    ].count(True)\n",
        "    num_richard_3s = [\n",
        "        text.split(\"RICHARD\")[1].strip() == \"III\" for text in sampled_texts\n",
        "    ].count(True)\n",
        "    gen_prob_richard_2 = num_richard_2s / len(sampled_texts)\n",
        "    gen_prob_richard_3 = num_richard_3s / len(sampled_texts)\n",
        "\n",
        "    print(f\"Probability of generating Richard II: {gen_prob_richard_2}\")\n",
        "    print(f\"Probability of generating Richard III: {gen_prob_richard_3}\")\n",
        "    if gen_prob_richard_2 < gen_prob_richard_3:\n",
        "        print(\"Test passed! :)\")\n",
        "    else:\n",
        "        print(\"Test failed! :(\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    print(\n",
        "        \"Test Case 4: Check if the probability of generating II given RICHARD  are close to the expected values\"\n",
        "    )\n",
        "    evaluate_test_case(\"Richard II\", gen_prob_richard_2, 0.35251798561151076, output_str=\"Probability of generating Richard II\", atol=1e-2)\n",
        "\n",
        "    print(\n",
        "        \"Test Case 5: Check if the probability of generating III given RICHARD are close to the expected values\"\n",
        "    )\n",
        "    evaluate_test_case(\"Richard III\", gen_prob_richard_3, 0.49640287769784175, output_str=\"Probability of generating Richard III\", atol=1e-2)\n",
        "\n",
        "test_sample_text_bigram_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAdVznHUa0Qj"
      },
      "outputs": [],
      "source": [
        "for _ in range(20):\n",
        "    sampled_text = bigram_lm.sample_text(\"<sos> KING\", max_words=50)\n",
        "    print(sampled_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9FrE4hja0Qk"
      },
      "source": [
        "The generations should look much better now, way more coherent compared to the unigram model! At least on the surface level it seems to capture the style of Shakespeare's writing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E23F-yNna0Qk"
      },
      "source": [
        "Test implementation of `eval_perplexity` for trigram, 4-gram, and 5-gram models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NX9vsgya0Qk"
      },
      "outputs": [],
      "source": [
        "trigram_lm = WordNGramLM(3)\n",
        "trigram_lm.fit(train_data)\n",
        "\n",
        "train_ppl = trigram_lm.eval_perplexity(train_data)\n",
        "dev_ppl = trigram_lm.eval_perplexity(dev_data)\n",
        "print(f\"Train Perplexity for Trigram model: {train_ppl}\")\n",
        "print(f\"Dev Perplexity for Trigram model: {dev_ppl}\")\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "fourgram_lm = WordNGramLM(4)\n",
        "fourgram_lm.fit(train_data)\n",
        "train_ppl = fourgram_lm.eval_perplexity(train_data)\n",
        "dev_ppl = fourgram_lm.eval_perplexity(dev_data)\n",
        "print(f\"Train Perplexity for 4-gram model: {train_ppl}\")\n",
        "print(f\"Dev Perplexity for 4-gram model: {dev_ppl}\")\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "fivegram_lm = WordNGramLM(5)\n",
        "fivegram_lm.fit(train_data)\n",
        "train_ppl = fivegram_lm.eval_perplexity(train_data)\n",
        "dev_ppl = fivegram_lm.eval_perplexity(dev_data)\n",
        "print(f\"Train Perplexity for 5-gram model: {train_ppl}\")\n",
        "print(f\"Dev Perplexity for 5-gram model: {dev_ppl}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ER4bx861a0Qk"
      },
      "source": [
        "You should see train perplexities approximately 5.9, 2.1, and 1.6 for trigram, 4-gram, and 5-gram LMs. For dev data the dev perplexity should be infinity for all the three."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldTT7G6na0Qk"
      },
      "source": [
        "Test implementation of `sample_text` for trigram and 4-gram LMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bo6rr7X_a0Ql"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "print(\"Generations from Trigram model\")\n",
        "for _ in range(20):\n",
        "    sampled_text = trigram_lm.sample_text(\"<sos> <sos> KING\", max_words=50)\n",
        "    print(sampled_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNyp5YVqa0Ql"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "print(\"Generations from 4-gram model\")\n",
        "for _ in range(20):\n",
        "    sampled_text = fourgram_lm.sample_text(\"<sos> <sos> <sos> KING\", max_words=50)\n",
        "    print(sampled_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K01hFSaKa0Ql"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "print(\"Generations from 5-gram model\")\n",
        "for _ in range(20):\n",
        "    sampled_text = fivegram_lm.sample_text(\"<sos> <sos> <sos> <sos> KING\", max_words=50)\n",
        "    print(sampled_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-Acd2R7m991"
      },
      "outputs": [],
      "source": [
        "# </NO_AUTOGRADE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9kLM-cma0Ql"
      },
      "source": [
        "You should see that the generation quality is now so much better. However, if you look closely (this is specially true for 4-gram and 5-gram models) that some of the sentences are directly lifted from the training data. Which makes sense, as we will increase the order of the N-gram LM, so does we increase its capacity and hence more the ability to memorize its training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orL3WGmma0Qm"
      },
      "source": [
        "## Part 3: Smoothing and Interpolation (27 Points)\n",
        "\n",
        "The issue with using N-gram language models is that any finite training corpus is bound to miss some N-grams that appear in the test set. The models hence assign zero probability to such N-grams, leading to probability of the entire test set to be zero and hence infinite perplexity values that we observed in the previous exercise.\n",
        "\n",
        "The standard way to deal with zero-probability N-gram tokens is to use smoothing algorithms. Smoothing algorithms shave off a bit of probability mass from some more frequent events and give it to unseen events. Smoothing algorithms for N-gram language models is a well studied area of research with numerous algorithms. For this exercise, we will focus on Laplace Smoothing and Interpolation.\n",
        "\n",
        "**Hint:** Try to use the inheritance from `WordNGramLM` to simplify your implementations for this part. You can use `super().method(arguments)` to call a method from the super class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CbTfIF0a0Qm"
      },
      "source": [
        "### Exercise 3.1 Laplace and Add-Lambda Smoothing (10 Points)\n",
        "\n",
        "Perhaps the simplest smoothing algorithm that exists is Laplace smoothing. It merely adds one to count of each N-gram, so that there is no zero-probability N-gram in the test data. For a bigram model, the expression for Laplace-smoothened distribution is given by:\n",
        "\n",
        "$$P_{\\text{Laplace}}(w_n \\mid w_{n-1}) = \\frac{C(w_{n-1}w_{n}) + 1}{\\sum_{w \\in \\mathcal{W}} (C(w_n w) + 1) } = \\frac{C(w_{n-1}w_{n}) + 1}{ C(w_{n-1}) + V }$$\n",
        "\n",
        "We can similarly write expressions for other N-gram models.\n",
        "\n",
        "Laplace smoothing is also called \"Add-one\" smoothing. A generalization of Laplace Smoothing is \"Add-k\" smoothing with  with $\\lambda < 1$, where we move a bit less of the probability mass from seen to unseen N-grams. The expression for Add-k smoothened distribution for bigram LM is given by:\n",
        "\n",
        "$$P_{\\text{Add-k}}(w_n \\mid w_{n -1}) = \\frac{C(w_{n-1}w_{n}) + \\lambda}{\\sum_{w \\in \\mathcal{W}} (C(w_n w) + \\lambda) } = \\frac{C(w_{n-1}w_{n}) + \\lambda}{ C(w_{n-1}) + \\lambda V }$$\n",
        "\n",
        "We will use the variable `k` to represent Lambda in the code.\n",
        "\n",
        "For this exercise, we ask you to implement `WordNGramLMWithAddKSmoothing` class. You need to follow the same code structure as `WordNGramLM` class, with only difference being in calculation of the conditional distributions.\n",
        "\n",
        "Note: It is no longer possible to implement an O(T + V) time complexity solution for `sample_text` function, hence we will accept O(TV) implementations here. Your `eval_perplexity` should still be as efficient as in the `WordNGramLM`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFxiq3XLa0Qm",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-34000a508fd844f8",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class WordNGramLMWithAddKSmoothing(WordNGramLM):\n",
        "    \"\"\"\n",
        "    Remember you can use the inheritance from WordNGramLM in your implementation!\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, N: int, k: int = 1):\n",
        "        super().__init__(N)\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, train_data: List[str]):\n",
        "        \"\"\"\n",
        "        Trains an N-gram language model with Add-k smoothing.\n",
        "\n",
        "        Inputs:\n",
        "            - train_data: str, sentences in the training data\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def eval_perplexity(self, eval_data: List[str]) -> float:\n",
        "        \"\"\"\n",
        "        Evaluates the perplexity of the N-gram language model with Add-k smoothing on the eval set.\n",
        "\n",
        "        Input:\n",
        "            - eval_data: List[str], the evaluation text\n",
        "\n",
        "        Output:\n",
        "            - float, the perplexity of the model on the evaluation set\n",
        "\n",
        "        Note : For tokens that are not in the vocabulary, replace them with the <unk> token.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def sample_text(\n",
        "        self, prefix: str = \"<sos>\", max_words: int = 100,\n",
        "    ) -> float:\n",
        "        \"\"\"\n",
        "        Samples text from the N-gram language model.\n",
        "\n",
        "        Inputs:\n",
        "            - prefix: str, the prefix to start the sampling from. Can also be multiple words separated by spaces.\n",
        "            - max_words: int, the maximum number of words to sample\n",
        "\n",
        "        Outputs:\n",
        "            - str, the sampled text\n",
        "\n",
        "        Note: Please use np.random.choice for sampling next words\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # Extra utility functions that you think will be useful can go below\n",
        "\n",
        "    # YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1B4ipnrPm991"
      },
      "outputs": [],
      "source": [
        "# <NO_AUTOGRADE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqPYhB7la0Qn"
      },
      "source": [
        "Test implementation of `eval_perplexity` for bigram model with Laplace smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxgAXJO6a0Qn"
      },
      "outputs": [],
      "source": [
        "# Test implementation of `eval_perplexity` for bigram model with Laplace smoothing\n",
        "bigram_lm = WordNGramLMWithAddKSmoothing(2, k = 1)\n",
        "bigram_lm.fit(train_data)\n",
        "train_ppl = bigram_lm.eval_perplexity(train_data)\n",
        "print(f\"Train Perplexity: {train_ppl}\")\n",
        "dev_ppl = bigram_lm.eval_perplexity(dev_data)\n",
        "print(f\"Dev Perplexity: {dev_ppl}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rJnykB2a0Qo"
      },
      "source": [
        "You should get a train perplexity around 404 and dev perplexity around 390. Notice how the dev perplexity is not $\\infty$ anymore! Though it comes at the cost of an increase in perplexity on training data, which is natural since the model now cut offs some probability mass from training N-grams and distribute it to the unseen ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aof4wXE7a0Qo"
      },
      "source": [
        "Test implementation of `eval_perplexity` for bigram model with Add-k smoothing (k = 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4Iq_arNa0Qo"
      },
      "outputs": [],
      "source": [
        "# Test implementation of `eval_perplexity` for bigram model with Add-k smoothing\n",
        "bigram_lm = WordNGramLMWithAddKSmoothing(2, k=0.01)\n",
        "bigram_lm.fit(train_data)\n",
        "train_ppl = bigram_lm.eval_perplexity(train_data)\n",
        "print(f\"Train Perplexity: {train_ppl}\")\n",
        "dev_ppl = bigram_lm.eval_perplexity(dev_data)\n",
        "print(f\"Dev Perplexity: {dev_ppl}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzTuUGj5a0Qp"
      },
      "source": [
        "You should get a train perplexity around 61 and a dev perplexity of roughly 148. Notice how we do much better on train perplexity when k is smaller, since now we re-distribute much less of the mass from the training N-grams. Luckily, in this case it turns out it improves the dev perplexity too."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRuwBCtUa0Qp"
      },
      "source": [
        "Test implementation of `sample_text` for bigram model with Laplace Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqYM-0vja0Qp"
      },
      "outputs": [],
      "source": [
        "def test_sample_text_bigram_laplace_model():\n",
        "    bigram_lm = WordNGramLMWithAddKSmoothing(2, k = 1)\n",
        "    bigram_lm.fit(train_data_wth_unks)\n",
        "\n",
        "    random.seed(42)\n",
        "    np.random.seed(42)\n",
        "    sampled_text = bigram_lm.sample_text(\"<sos>\", max_words=50)\n",
        "\n",
        "    print(\"Test Case 1: Check if the sampled text starts with <sos>\")\n",
        "    evaluate_test_case(\n",
        "        None,\n",
        "        sampled_text.startswith(\"<sos>\"),\n",
        "        True,\n",
        "        output_str=\"Sampled text starts with <sos>\",\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        \"Test Case 2: Check if the number of generated words is either 50 or less than 50 and ends with <eos>\"\n",
        "    )\n",
        "    print(f\"Generated text: {sampled_text}\")\n",
        "    print(f\"Number of generated words: {len(sampled_text.split()) - 1}\")\n",
        "    print(f\"Does the generated text end with <eos>: {'<eos>' in sampled_text}\")\n",
        "    if len(sampled_text.split()) - 1 == 50 or (\n",
        "        len(sampled_text.split()) < 50 and \"<eos>\" in sampled_text\n",
        "    ):\n",
        "        print(\"Test passed! :)\")\n",
        "    else:\n",
        "        print(\"Test failed! :(\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
        "\n",
        "    print(\n",
        "        \"Test Case 3: Check if the probability of generating II is greater than III when prefix is RICHARD\"\n",
        "    )\n",
        "    sampled_texts = [\n",
        "        bigram_lm.sample_text(\"RICHARD\", max_words=1) for _ in range(1000)\n",
        "    ]\n",
        "    sampled_text = \" \".join(sampled_texts)\n",
        "    num_richard_2s = [\n",
        "        text.split(\"RICHARD\")[1].strip() == \"II\" for text in sampled_texts\n",
        "    ].count(True)\n",
        "    num_richard_3s = [\n",
        "        text.split(\"RICHARD\")[1].strip() == \"III\" for text in sampled_texts\n",
        "    ].count(True)\n",
        "    gen_prob_richard_2 = num_richard_2s / len(sampled_texts)\n",
        "    gen_prob_richard_3 = num_richard_3s / len(sampled_texts)\n",
        "\n",
        "    print(f\"Probability of generating Richard II: {gen_prob_richard_2}\")\n",
        "    print(f\"Probability of generating Richard III: {gen_prob_richard_3}\")\n",
        "    if gen_prob_richard_2 < gen_prob_richard_3:\n",
        "        print(\"Test passed! :)\")\n",
        "    else:\n",
        "        print(\"Test failed! :(\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
        "\n",
        "    print(\n",
        "        \"Test Case 4: Check if the probability of generating II given RICHARD  are close to the expected values\"\n",
        "    )\n",
        "    evaluate_test_case(\n",
        "        \"Richard II\",\n",
        "        gen_prob_richard_2,\n",
        "        0.014,\n",
        "        output_str=\"Probability of generating Richard II\",\n",
        "        atol=1e-3,\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        \"Test Case 5: Check if the probability of generating III given RICHARD are close to the expected values\"\n",
        "    )\n",
        "    evaluate_test_case(\n",
        "        \"Richard III\",\n",
        "        gen_prob_richard_3,\n",
        "        0.034,\n",
        "        output_str=\"Probability of generating Richard III\",\n",
        "        atol=1e-3,\n",
        "    )\n",
        "\n",
        "test_sample_text_bigram_laplace_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9tkvi5ta0Qp"
      },
      "outputs": [],
      "source": [
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "bigram_lm = WordNGramLMWithAddKSmoothing(2, k = 1)\n",
        "bigram_lm.fit(train_data)\n",
        "for _ in range(20):\n",
        "    sampled_text = bigram_lm.sample_text(\"<sos> KING\", max_words=50)\n",
        "    print(sampled_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KrHpXBRa0Qp"
      },
      "source": [
        "You should see generations which are much less like training data! <sos> KING is now followed by words other than name of king names like before. Though the generations are much less clean now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSoMzlqda0Qp"
      },
      "outputs": [],
      "source": [
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "bigram_lm = WordNGramLMWithAddKSmoothing(2, k=0.01)\n",
        "bigram_lm.fit(train_data)\n",
        "for _ in range(20):\n",
        "    sampled_text = bigram_lm.sample_text(\"<sos> KING\", max_words=50)\n",
        "    print(sampled_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNehHdLla0Qq"
      },
      "source": [
        "Notice how with a smaller value of $k$, the generated sentences now start to resemble more with the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dWDvo-pa0Qq"
      },
      "source": [
        "Test implementation of `eval_perplexity` for trigram, 4-gram, and 5-gram LMs with Laplace smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLjn8Fn8a0Qq"
      },
      "outputs": [],
      "source": [
        "trigram_lm = WordNGramLMWithAddKSmoothing(3, k =1)\n",
        "trigram_lm.fit(train_data_wth_unks)\n",
        "\n",
        "train_ppl = trigram_lm.eval_perplexity(train_data_wth_unks)\n",
        "dev_ppl = trigram_lm.eval_perplexity(dev_data)\n",
        "print(f\"Train Perplexity for Trigram model: {train_ppl}\")\n",
        "print(f\"Dev Perplexity for Trigram model: {dev_ppl}\")\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "fourgram_lm = WordNGramLMWithAddKSmoothing(4, k =1)\n",
        "fourgram_lm.fit(train_data_wth_unks)\n",
        "train_ppl = fourgram_lm.eval_perplexity(train_data_wth_unks)\n",
        "dev_ppl = fourgram_lm.eval_perplexity(dev_data)\n",
        "print(f\"Train Perplexity for 4-gram model: {train_ppl}\")\n",
        "print(f\"Dev Perplexity for 4-gram model: {dev_ppl}\")\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "fivegram_lm = WordNGramLMWithAddKSmoothing(5, k =1)\n",
        "fivegram_lm.fit(train_data_wth_unks)\n",
        "train_ppl = fivegram_lm.eval_perplexity(train_data_wth_unks)\n",
        "dev_ppl = fivegram_lm.eval_perplexity(dev_data)\n",
        "print(f\"Train Perplexity for 5-gram model: {train_ppl}\")\n",
        "print(f\"Dev Perplexity for 5-gram model: {dev_ppl}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDdtSkNEa0Qq"
      },
      "source": [
        "You should roughly observe the following perplexities:\n",
        "\n",
        "|N-gram LM | Train Perplexity | Dev Perplexity|\n",
        "|----------|------------------|---------------|\n",
        "| Trigram   | 1276              | 1829           |\n",
        "| 4-gram  | 1710              | 3041         |\n",
        "| 5-gram | 1794 | 3339 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YK213wVKa0Qq"
      },
      "source": [
        "Test implementation of `sample_text` for trigram and 4-gram LMs with Laplace Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gc7WlvFXa0Qq"
      },
      "outputs": [],
      "source": [
        "def test_sample_text_ngram_laplace():\n",
        "    print(\"Testing for Trigram model\")\n",
        "\n",
        "    random.seed(42)\n",
        "    np.random.seed(42)\n",
        "    trigram_lm = WordNGramLMWithAddKSmoothing(3)\n",
        "    trigram_lm.fit(train_data)\n",
        "    sampled_text = trigram_lm.sample_text(\"<sos> <sos>\", max_words=50)\n",
        "\n",
        "    print(\"Test Case 1: Check if the sampled text starts with <sos> <sos>\")\n",
        "    evaluate_test_case(\n",
        "        None,\n",
        "        sampled_text.startswith(\"<sos> <sos>\"),\n",
        "        True,\n",
        "        output_str=\"Sampled text starts with <sos> <sos>\",\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        \"Test Case 2: Check if the number of generated words is either 50 or less than 50 and ends with <eos>\"\n",
        "    )\n",
        "    print(f\"Generated text: {sampled_text}\")\n",
        "    print(f\"Number of generated words: {len(sampled_text.split()) - 2}\")\n",
        "    print(f\"Does the generated text end with <eos>: {'<eos>' in sampled_text}\")\n",
        "    if len(sampled_text.split()) - 2 == 50 or (\n",
        "        len(sampled_text.split()) - 2 < 50 and \"<eos>\" in sampled_text\n",
        "    ):\n",
        "        print(\"Test passed! :)\")\n",
        "    else:\n",
        "        print(\"Test failed! :(\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
        "\n",
        "    print(\n",
        "        \"Test Case 3: Check if the probability of generating II is less than III when prefix is KING RICHARD\"\n",
        "    )\n",
        "    sampled_texts = [\n",
        "        trigram_lm.sample_text(\"KING RICHARD\", max_words=1) for _ in range(10000)\n",
        "    ]\n",
        "    sampled_text = \" \".join(sampled_texts)\n",
        "    num_richard_2s = [\n",
        "        text.split(\"KING RICHARD\")[1].strip() == \"II\" for text in sampled_texts\n",
        "    ].count(True)\n",
        "    num_richard_3s = [\n",
        "        text.split(\"KING RICHARD\")[1].strip() == \"III\" for text in sampled_texts\n",
        "    ].count(True)\n",
        "\n",
        "    gen_prob_richard_2 = num_richard_2s / len(sampled_texts)\n",
        "    gen_prob_richard_3 = num_richard_3s / len(sampled_texts)\n",
        "\n",
        "    print(f\"Probability of generating Richard II: {gen_prob_richard_2}\")\n",
        "    print(f\"Probability of generating Richard III: {gen_prob_richard_3}\")\n",
        "    if gen_prob_richard_2 < gen_prob_richard_3:\n",
        "        print(\"Test passed! :)\")\n",
        "    else:\n",
        "        print(\"Test failed! :(\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
        "\n",
        "    print(\n",
        "        \"Test Case 4: Check if the probability of generating II given KING RICHARD are close to the expected values\"\n",
        "    )\n",
        "    evaluate_test_case(\n",
        "        \"King Richard II\",\n",
        "        gen_prob_richard_2,\n",
        "        0.0201,\n",
        "        output_str=\"Probability of generating Richard II\",\n",
        "        atol=1e-2,\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        \"Test Case 5: Check if the probability of generating III given KING RICHARD are close to the expected values\"\n",
        "    )\n",
        "    evaluate_test_case(\n",
        "        \"King Richard III\",\n",
        "        gen_prob_richard_3,\n",
        "        0.0306,\n",
        "        output_str=\"Probability of generating Richard III\",\n",
        "        atol=1e-2,\n",
        "    )\n",
        "\n",
        "    print(\"Testing for 4-gram model\")\n",
        "    fourgram_lm = WordNGramLMWithAddKSmoothing(4)\n",
        "    fourgram_lm.fit(train_data)\n",
        "    sampled_text = fourgram_lm.sample_text(\"<sos> <sos> <sos>\", max_words=50)\n",
        "\n",
        "    print(\"Test Case 6: Check if the sampled text starts with <sos> <sos> <sos>\")\n",
        "    evaluate_test_case(\n",
        "        None,\n",
        "        sampled_text.startswith(\"<sos> <sos> <sos>\"),\n",
        "        True,\n",
        "        output_str=\"Sampled text starts with <sos> <sos> <sos>\",\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        \"Test Case 7: Check if the number of generated words is either 50 or less than 50 and ends with <eos>\"\n",
        "    )\n",
        "    print(f\"Generated text: {sampled_text}\")\n",
        "    print(f\"Number of generated words: {len(sampled_text.split()) - 3}\")\n",
        "    print(f\"Does the generated text end with <eos>: {'<eos>' in sampled_text}\")\n",
        "    if len(sampled_text.split()) - 3 == 50 or (\n",
        "        len(sampled_text.split()) - 3 < 50 and \"<eos>\" in sampled_text\n",
        "    ):\n",
        "        print(\"Test passed! :)\")\n",
        "\n",
        "    else:\n",
        "        print(\"Test failed! :(\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
        "\n",
        "    print(\n",
        "        \"Test Case 8: Check if the probability of generating II is less than III when prefix is <sos> KING RICHARD\"\n",
        "    )\n",
        "    sampled_texts = [\n",
        "        fourgram_lm.sample_text(\"<sos> <sos> KING RICHARD\", max_words=1)\n",
        "        for _ in range(1000)\n",
        "    ]\n",
        "    sampled_text = \" \".join(sampled_texts)\n",
        "    num_richard_2s = [\n",
        "        text.split(\"<sos> <sos> KING RICHARD\")[1].strip() == \"II\"\n",
        "        for text in sampled_texts\n",
        "    ].count(True)\n",
        "\n",
        "    num_richard_3s = [\n",
        "        text.split(\"<sos> <sos> KING RICHARD\")[1].strip() == \"III\"\n",
        "        for text in sampled_texts\n",
        "    ].count(True)\n",
        "\n",
        "    gen_prob_rich2 = num_richard_2s / len(sampled_texts)\n",
        "    gen_prob_rich3 = num_richard_3s / len(sampled_texts)\n",
        "\n",
        "    print(f\"Probability of generating Richard II: {gen_prob_rich2}\")\n",
        "    print(f\"Probability of generating Richard III: {gen_prob_rich3}\")\n",
        "    if gen_prob_rich2 < gen_prob_rich3:\n",
        "        print(\"Test passed! :)\")\n",
        "    else:\n",
        "        print(\"Test failed! :(\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
        "\n",
        "    print(\n",
        "        \"Test Case 9: Check if the probability of generating II given <sos> KING RICHARD are close to the expected values\"\n",
        "    )\n",
        "    evaluate_test_case(\n",
        "        \"<sos> King Richard II\",\n",
        "        gen_prob_rich2,\n",
        "        0.0174,\n",
        "        output_str=\"Probability of generating Richard II\",\n",
        "        atol=1e-2,\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        \"Test Case 10: Check if the probability of generating III given <sos> KING RICHARD are close to the expected values\"\n",
        "    )\n",
        "    evaluate_test_case(\n",
        "        \"<sos> King Richard III\",\n",
        "        gen_prob_rich3,\n",
        "        0.0295,\n",
        "        output_str=\"Probability of generating Richard III\",\n",
        "        atol=1e-2,\n",
        "    )\n",
        "test_sample_text_ngram_laplace()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7bXoUsEa0Qr"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "print(\"Generations from Trigram model\")\n",
        "trigram_lm = WordNGramLMWithAddKSmoothing(3, k = 0.01)\n",
        "trigram_lm.fit(train_data)\n",
        "for _ in range(20):\n",
        "    sampled_text = trigram_lm.sample_text(\"<sos> <sos> KING\", max_words=50)\n",
        "    print(sampled_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfBKbbqCa0Qr"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "print(\"Generations from 4-gram model with Add-k smoothing\")\n",
        "fourgram_lm = WordNGramLMWithAddKSmoothing(4, k=0.01)\n",
        "fourgram_lm.fit(train_data)\n",
        "for _ in range(20):\n",
        "    sampled_text = fourgram_lm.sample_text(\"<sos> <sos> <sos> KING\", max_words=50)\n",
        "    print(sampled_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oia_0kuRa0Qr"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "print(\"Generations from 5-gram model with Add-k smoothing\")\n",
        "fivegram_lm = WordNGramLMWithAddKSmoothing(5, k=0.01)\n",
        "fivegram_lm.fit(train_data)\n",
        "for _ in range(20):\n",
        "    sampled_text = fivegram_lm.sample_text(\"<sos> <sos> <sos> <sos> KING\", max_words=50)\n",
        "    print(sampled_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbrkRzPom992"
      },
      "outputs": [],
      "source": [
        "# </NO_AUTOGRADE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fbCENTKa0Qr"
      },
      "source": [
        "### Write-Up Question 4 (1 Point)\n",
        "\n",
        "Write expression for unigram, trigram, 4-gram, and 5-gram models with Laplace smoothing. (Your answer to this question should go in your separate write-up PDF.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDcHnKVQa0Qr"
      },
      "source": [
        "### Write-Up Question 5 (3 Points)\n",
        "\n",
        "Plot how the train and dev perplexities of bigram, trigram, 4-gram, and 5-gram LMs with Add-k smoothing from different values of k -- $\\{1e-8, 1e-7, \\cdots, 1e-1, 1\\}$. You should have perplexity on the y-axis and k on the x axis. Use log-scaling for the x axis when plotting. Explain the trend that you see in 3 lines. Finally, report the best setup i.e. values of $N$ and $k$ which achieve the best dev accuracy. (Your answer to this question should go in your separate write-up PDF; the following code block below is for the computation you'll need to do in preparation for your write-up.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# <NO_AUTOGRADE>"
      ],
      "metadata": {
        "id": "APrxWb8zSDcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7w1drLYa0Qs"
      },
      "outputs": [],
      "source": [
        "\n",
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# </NO_AUTOGRADE>"
      ],
      "metadata": {
        "id": "dGZixnjpSFC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKhYDyyVa0Qs"
      },
      "source": [
        "### Exercise 3.3 Language Model Interpolation (10 Points)\n",
        "\n",
        "An alternate to smoothing that often works well in practice is interpolating between different language models. Let's say we are trying to compute $P(w_n \\mid w_{n-2} w_{n-1})$, but we have no examples of the particular trigram $w_{n-2}, w_{n-1} w_n$ in the training corpus, we can instead estimate its probability by using the bigram probability $P(w_n \\mid w_{n-1})$. If there are no examples of the bigram $w_{n-1} w_n$ in the training data either, we use the unigram probability $P(w_n)$. Formally, the trigram probability by mixing the three distributions is given by:\n",
        "\n",
        "$$\\hat{P}(w_n \\mid w_{n-2} w_{n-1}) = \\lambda_1 P(w_n) + \\lambda_2 P(w_n \\mid w_{n-1}) + \\lambda_3 P(w_n \\mid w_{n-1} w_{n-2})$$\n",
        "\n",
        "where $\\lambda_1 + \\lambda_2 + \\lambda_3 = 1$ (and each $\\lambda$ is non-negative), making the above equation a form of weighted averaging. We can similarly write expressions for other N-gram LMs.\n",
        "\n",
        "But how do we choose the values of different $\\lambda_i$? We choose these values by tuning them on a held out data i.e. the dev set, very similar to tuning hyperparameters for a machine learning model.\n",
        "\n",
        "In this exercise, you will implement the class `WordNGramLMWithInterpolation` similar to `WordNGramLM` and `WordNGramLMWithAddKSmoothing` that you did in the previous exercises but this time to support interpolation between different LMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uv1jZkvEa0Qv",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-2bfd0cbf822202bb",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class WordNGramLMWithInterpolation(WordNGramLM):\n",
        "    \"\"\"\n",
        "    Remember you can use the inheritance from WordNGramLM in your implementation!\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, N: int, lambdas: List[float]):\n",
        "\n",
        "        \"\"\"\n",
        "        Constructor for WordNGramLMWithInterpolation class.\n",
        "        Inputs:\n",
        "            - N: int, the N in N-gram\n",
        "            - lambdas: List[float], the list of lambdas for interpolation between 1-gram, 2-gram, 3-gram, ..., N-gram models\n",
        "                Note: The length of lambdas should be N. The sum of lambdas should be 1. lambdas[0] corresponds to 1-gram model, lambdas[1] corresponds to 2-gram model and so on.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def fit(self, train_data: List[str]):\n",
        "\n",
        "        \"\"\"\n",
        "        Trains an N-gram language model with interpolation.\n",
        "\n",
        "        Inputs:\n",
        "            - train_data: str, sentences in the training data\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def eval_perplexity(self, eval_data: List[str]) -> float:\n",
        "        \"\"\"\n",
        "        Evaluates the perplexity of the N-gram language model with interpolation on the eval set.\n",
        "\n",
        "        Input:\n",
        "            - eval_data: List[str], the evaluation text\n",
        "\n",
        "        Output:\n",
        "            - float, the perplexity of the model on the evaluation set\n",
        "\n",
        "        Note : For tokens that are not in the vocabulary, replace them with the <unk> token.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def sample_text(self, prefix: str = \"<sos>\", max_words: int = 100) -> float:\n",
        "\n",
        "        \"\"\"\n",
        "        Samples text from the N-gram language model with interpolation.\n",
        "\n",
        "        Inputs:\n",
        "            - prefix: str, the prefix to start the sampling from. Can also be multiple words separated by spaces.\n",
        "            - max_words: int, the maximum number of words to sample\n",
        "\n",
        "        Outputs:\n",
        "            - str, the sampled text\n",
        "\n",
        "        Note: Please use np.random.choice for sampling next words\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # Extra utility functions that you think will be useful can go below\n",
        "\n",
        "    # YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVw2Oea9m992"
      },
      "outputs": [],
      "source": [
        "# <NO_AUTOGRADE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCZ-a3tka0Qv"
      },
      "source": [
        "Test implementation of `eval_perplexity` for trigram model with Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R300QdoKa0Qv"
      },
      "outputs": [],
      "source": [
        "trigram_lm = WordNGramLMWithInterpolation(3, [0.3, 0.3, 0.4])\n",
        "trigram_lm.fit(train_data_wth_unks)\n",
        "\n",
        "train_ppl = trigram_lm.eval_perplexity(train_data_wth_unks)\n",
        "dev_ppl = trigram_lm.eval_perplexity(dev_data)\n",
        "print(f\"Train Perplexity for Trigram model with Interpolation: {train_ppl}\")\n",
        "print(f\"Dev Perplexity for Trigram model with Interpolation: {dev_ppl}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhwkOSWTa0Qv"
      },
      "source": [
        "You should see a train perplexity of around 13 and dev perplexity of 139. This should also be the best performing model based on dev perplexity that we have seen so far."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vHnynkWa0Qw"
      },
      "source": [
        "Test implementation of `eval_perplexity` for 4-gram model with Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vhx4LBa4a0Qw"
      },
      "outputs": [],
      "source": [
        "fourgram_lm = WordNGramLMWithInterpolation(4, [0.2, 0.4, 0.3, 0.1])\n",
        "fourgram_lm.fit(train_data_wth_unks)\n",
        "\n",
        "train_ppl = fourgram_lm.eval_perplexity(train_data_wth_unks)\n",
        "dev_ppl = fourgram_lm.eval_perplexity(dev_data)\n",
        "print(f\"Train Perplexity for Trigram model with Interpolation: {train_ppl}\")\n",
        "print(f\"Dev Perplexity for Trigram model with Interpolation: {dev_ppl}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj-5Q1Wga0Qw"
      },
      "source": [
        "Here you should get a train perplexity of roughly 8 and dev perplexity around 144."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhCEXY4ca0Qw"
      },
      "source": [
        "Play around with different values of $\\lambda_i$ and see how it effects the train and dev perplexities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhjnmHvqa0Qw"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(\"Generations from Trigram model with Interpolation\")\n",
        "trigram_lm = WordNGramLMWithInterpolation(3, [0.3, 0.3, 0.4])\n",
        "trigram_lm.fit(train_data)\n",
        "for _ in range(20):\n",
        "    sampled_text = trigram_lm.sample_text(\"<sos> <sos> KING\", max_words=50)\n",
        "    print(sampled_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkud3ZqVa0Qw"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(\"Generations from 4-gram model with Interpolation\")\n",
        "fourgram_lm = WordNGramLMWithInterpolation(4, [0.2, 0.4, 0.3, 0.1])\n",
        "fourgram_lm.fit(train_data)\n",
        "for _ in range(20):\n",
        "    sampled_text = fourgram_lm.sample_text(\"<sos> <sos> <sos> KING\", max_words=50)\n",
        "    print(sampled_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yebBSzHya0Qw"
      },
      "source": [
        "### Write-Up Question 6 (3 Points)\n",
        "\n",
        "For bigram, trigram, and 4-gram models with interpolation, train models with different values of $\\lambda_i$ and evaluate perplexities on train and dev datasets. You should try at least 5 sets of values for each model. Report the $\\lambda_i$ values that you experiment with and train and dev perplexities for each setting and N-gram model. (Your answer to this question should go in your separate write-up PDF; the following code block below is for the computation you'll need to do in preparation for your write-up.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVv5uzW7a0Qx"
      },
      "outputs": [],
      "source": [
        "\n",
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "147lKMkPm993"
      },
      "outputs": [],
      "source": [
        "# </NO_AUTOGRADE>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "au24-cse-447",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}