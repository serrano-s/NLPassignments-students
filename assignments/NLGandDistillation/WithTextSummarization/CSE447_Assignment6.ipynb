{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkjrCuRHZ4So"
      },
      "source": [
        "# Assignment 6: Natural Language Generation Generation\n",
        "\n",
        "In this part of the homework, you will implement decoding algorithms covered in class -- greedy decoding, random sampling, temperature sampling, top-k sampling, and top-p (nucleus) sampling. You will also learn how to use knowledge distillation to use strong teacher models to improve performance of weaker student models. The knowledge distillation exercise is only mandatory for CSE 517 students.\n",
        "\n",
        "Notes about the autograder for this assignment:\n",
        "\n",
        "- To submit the coding part of this assignment to Gradescope, download your .ipynb notebook in .ipynb form and submit **only** this file. It must be named **CSE447_Assignment6.ipynb** in order for the autograder to work.\n",
        "- At a low-load time on Gradescope, the autograder for this assignment takes about **two minutes** to run. We recommend submitting the code part of your assignment with enough time before the deadline to allow the autograder to run, and for you to investigate/correct any issues it flags."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5G_qkrRCZ4Sp"
      },
      "source": [
        "## Section 0: Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# <NO_AUTOGRADE> <=== Leave this code cell as is, otherwise the autograder will have trouble processing your submission"
      ],
      "metadata": {
        "id": "h71lYdnJeWVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mc6c5byhCKjr"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "# get any other necessary files for this project\n",
        "if [ ! -e \"cache/\" ]; then\n",
        "  mkdir cache\n",
        "fi\n",
        "\n",
        "if [ ! -e \"data-needed.txt\" ]; then\n",
        "  if [ ! -e \"data_path_to_download_url.py\" ]; then\n",
        "    wget https://raw.githubusercontent.com/serrano-s/NLPassignments-students/refs/heads/main/data_path_to_download_url.py\n",
        "  else\n",
        "    echo \"data_path_to_download_url.py script already downloaded to runtime\"\n",
        "  fi\n",
        "\n",
        "  wget https://raw.githubusercontent.com/serrano-s/NLPassignments-students/refs/heads/main/assignments/NLGandDistillation/WithTextSummarization/data-needed.txt\n",
        "\n",
        "  # download all data files needed for the student-release version of this project (i.e., no hidden test files)\n",
        "  DATA_NEEDED_FILE=\"data-needed.txt\"\n",
        "  closing_slash=\"/\"\n",
        "  while IFS= read -r line; do\n",
        "    line=\"$(echo -e \"${line}\" | sed -e 's/^[[:space:]]*//' -e 's/[[:space:]]*$//')\";\n",
        "    dirs_to_check=\"${line%${closing_slash}*}\"\n",
        "    mkdir -p $dirs_to_check\n",
        "    download_url=$(python data_path_to_download_url.py \"$line\")\n",
        "    echo $download_url;\n",
        "    wget \"$download_url\" -O \"$line\"\n",
        "  done < \"$DATA_NEEDED_FILE\"\n",
        "else\n",
        "  echo \"data-needed.txt (and presumably therefore all necessary data files) already downloaded to runtime\"\n",
        "fi\n",
        "\n",
        "if [ ! -e \"other-setup-needed.sh\" ]; then\n",
        "  wget https://raw.githubusercontent.com/serrano-s/NLPassignments-students/refs/heads/main/assignments/NLGandDistillation/WithTextSummarization/other-setup-needed.sh\n",
        "  bash other-setup-needed.sh\n",
        "  rm data_path_to_download_url.py\n",
        "else\n",
        "  echo \"other-setup-needed.sh (and presumably therefore all other necessary files) already downloaded to runtime\"\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMxKMBGlZ4Sp"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "pip install transformers==4.57.1 sentencepiece==0.2.1 protobuf==5.29.5 datasets==4.0.0 evaluate==0.4.6"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Leave this code cell as is, otherwise the autograder will have trouble processing your submission\n",
        "# </NO_AUTOGRADE>"
      ],
      "metadata": {
        "id": "Iyww_viHemja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNdigPssZ4Sp"
      },
      "outputs": [],
      "source": [
        "\"\"\"set device and random seeds\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper functions are given to you.\n",
        "######################################################\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "# Don't change the next four lines of code, the autograder needs them as-is\n",
        "if __name__ == '__main__':\n",
        "    from tqdm.notebook import tqdm\n",
        "else:\n",
        "    from tqdm import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DEVICE = device\n",
        "print(f\"device: {device}\")\n",
        "\n",
        "\n",
        "def set_seed(seed=19260817):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "set_seed()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uxgM1wQZ4Sq"
      },
      "source": [
        "### 0.1 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psVAeYyTZ4Sq"
      },
      "outputs": [],
      "source": [
        "\"\"\"load datasets\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    from datasets import load_dataset\n",
        "\n",
        "    dataset = load_dataset(\"Ximing/ROCStories\")\n",
        "    train_data, dev_data, test_data = (\n",
        "        dataset[\"train\"],\n",
        "        dataset[\"validation\"],\n",
        "        dataset[\"test\"],\n",
        "    )\n",
        "\n",
        "    print(train_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sGb0WzRZ4Sq"
      },
      "source": [
        "### 0.2 Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQha26laZ4Sq"
      },
      "outputs": [],
      "source": [
        "\"\"\"prepare evaluation\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    from evaluate import load\n",
        "    from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
        "\n",
        "    perplexity_scorer = load(\"perplexity\", module_type=\"metric\")\n",
        "    cola_model_name = \"textattack/roberta-base-CoLA\"\n",
        "    cola_tokenizer = RobertaTokenizer.from_pretrained(cola_model_name)\n",
        "    cola_model = RobertaForSequenceClassification.from_pretrained(cola_model_name).to(\n",
        "        device\n",
        "    )\n",
        "\n",
        "def batchify(data, batch_size):\n",
        "    assert batch_size > 0\n",
        "\n",
        "    batch = []\n",
        "    for item in data:\n",
        "        # Yield next batch\n",
        "        if len(batch) == batch_size:\n",
        "            yield batch\n",
        "            batch = []\n",
        "\n",
        "        batch.append(item)\n",
        "\n",
        "    # Yield last un-filled batch\n",
        "    if len(batch) != 0:\n",
        "        yield batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huvbVy9VZ4Sq"
      },
      "outputs": [],
      "source": [
        "\"\"\"set up evaluation metric\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "\n",
        "def compute_perplexity(texts, model=\"gpt2\", batch_size=8):\n",
        "    score = perplexity_scorer.compute(\n",
        "        predictions=texts, add_start_token=True, batch_size=batch_size, model_id=model\n",
        "    )\n",
        "    return score[\"mean_perplexity\"]\n",
        "\n",
        "\n",
        "def compute_fluency(texts, batch_size=8):\n",
        "    scores = []\n",
        "    for b_texts in batchify(texts, batch_size):\n",
        "        inputs = cola_tokenizer(\n",
        "            texts, padding=True, truncation=True, return_tensors=\"pt\"\n",
        "        ).to(device)\n",
        "        with torch.no_grad():\n",
        "            logits = cola_model(**inputs).logits\n",
        "            probs = logits.softmax(dim=-1)\n",
        "            scores.extend(probs[:, 1].tolist())\n",
        "    return sum(scores) / len(scores)\n",
        "\n",
        "\n",
        "def compute_diversity(texts):\n",
        "    unigrams, bigrams, trigrams = [], [], []\n",
        "    total_words = 0\n",
        "    for gen in texts:\n",
        "        o = gen.split(\" \")\n",
        "        total_words += len(o)\n",
        "        for i in range(len(o)):\n",
        "            unigrams.append(o[i])\n",
        "        for i in range(len(o) - 1):\n",
        "            bigrams.append(o[i] + \"_\" + o[i + 1])\n",
        "        for i in range(len(o) - 2):\n",
        "            trigrams.append(o[i] + \"_\" + o[i + 1] + \"_\" + o[i + 2])\n",
        "    return (\n",
        "        len(set(unigrams)) / len(unigrams),\n",
        "        len(set(bigrams)) / len(bigrams),\n",
        "        len(set(trigrams)) / len(trigrams),\n",
        "    )\n",
        "\n",
        "\n",
        "def evaluate(generations, experiment):\n",
        "    generations = [_ for _ in generations if _ != \"\"]\n",
        "    perplexity = compute_perplexity(generations)\n",
        "    fluency = compute_fluency(generations)\n",
        "    diversity = compute_diversity(generations)\n",
        "    print(experiment)\n",
        "    print(f\"perplexity = {perplexity:.2f}\")\n",
        "    print(f\"fluency = {fluency:.2f}\")\n",
        "    print(f\"diversity = {diversity[0]:.2f}, {diversity[1]:.2f}, {diversity[2]:.2f}\")\n",
        "    print()\n",
        "\n",
        "\n",
        "debug_sents = [\n",
        "    \"This restaurant is awesome\",\n",
        "    \"My dog is cute and I love it.\",\n",
        "    \"Today is sunny.\",\n",
        "]\n",
        "if __name__ == '__main__':\n",
        "    evaluate(debug_sents, \"debugging run\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvouVcqcZ4Sq"
      },
      "source": [
        "### 0.3: Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrHA1WcKZ4Sq"
      },
      "outputs": [],
      "source": [
        "\"\"\"load model and tokenizer\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, DynamicCache\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_name = \"gpt2\"\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name, pad_token=\"<|endoftext|>\")\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "    model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAAsjeysZ4Sr"
      },
      "source": [
        "## **Section 1: Decoding Algorithms**\n",
        "\n",
        "In this section, you will implement a few basic decoding algorithms:\n",
        "1. Greedy decoding\n",
        "2. Vanilla sampling\n",
        "3. Temperature sampling\n",
        "4. Top-k sampling\n",
        "5. Top-p sampling\n",
        "\n",
        "We have provided a wrapper function `decode()` that takes care of batching, controlling max length, and handling the EOS token.\n",
        "You will be asked to implement the core function of each method: *given the pre-softmax logits of the next token, decide what the next token is.*\n",
        "\n",
        "**The wrapper calls the core function of each decoding algorithm, which you will implement in the subsections below.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bidXO50XZ4Sr"
      },
      "outputs": [],
      "source": [
        "\"\"\"decode main wrapper function\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "def _update_model_kwargs_for_generation(\n",
        "    outputs, model_kwargs, is_encoder_decoder: bool = False\n",
        "):\n",
        "    # update past\n",
        "    if \"past_key_values\" in outputs:\n",
        "        model_kwargs[\"past_key_values\"] = outputs.past_key_values\n",
        "    elif \"mems\" in outputs:\n",
        "        model_kwargs[\"past_key_values\"] = outputs.mems\n",
        "    elif \"past_buckets_states\" in outputs:\n",
        "        model_kwargs[\"past_key_values\"] = outputs.past_buckets_states\n",
        "    else:\n",
        "        model_kwargs[\"past_key_values\"] = None\n",
        "\n",
        "    # update token_type_ids with last value\n",
        "    if \"token_type_ids\" in model_kwargs:\n",
        "        token_type_ids = model_kwargs[\"token_type_ids\"]\n",
        "        model_kwargs[\"token_type_ids\"] = torch.cat([token_type_ids, token_type_ids[:, -1].unsqueeze(-1)], dim=-1)\n",
        "\n",
        "    # update attention mask\n",
        "    if not is_encoder_decoder:\n",
        "        if \"attention_mask\" in model_kwargs:\n",
        "            attention_mask = model_kwargs[\"attention_mask\"]\n",
        "            model_kwargs[\"attention_mask\"] = torch.cat(\n",
        "                [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\n",
        "            )\n",
        "\n",
        "    return model_kwargs\n",
        "\n",
        "def decode(prompts, max_len, method, **kwargs):\n",
        "    encodings_dict = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
        "    input_ids = encodings_dict[\"input_ids\"].to(device)\n",
        "    attention_mask = encodings_dict[\"attention_mask\"].to(device)\n",
        "\n",
        "    cache_position = torch.arange(input_ids.shape[1], dtype=torch.int64, device=device)\n",
        "\n",
        "    model_kwargs = {\"attention_mask\": attention_mask}\n",
        "    batch_size, input_seq_len = input_ids.shape\n",
        "    past_key_values = DynamicCache()\n",
        "\n",
        "    unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=device)\n",
        "\n",
        "    for step in range(max_len):\n",
        "        dict_of_inputs = {'past_key_values': past_key_values}\n",
        "        dict_of_inputs.update(model_kwargs)\n",
        "        if 'cache_position' not in dict_of_inputs or dict_of_inputs['cache_position'] is None:\n",
        "            dict_of_inputs['cache_position'] = cache_position\n",
        "        model_inputs = model.prepare_inputs_for_generation(input_ids, **dict_of_inputs)\n",
        "\n",
        "        if 'cache_position' in model_inputs and model_inputs['cache_position'] is not None:\n",
        "            dict_of_inputs = {'past_key_values': past_key_values}\n",
        "            dict_of_inputs.update(model_inputs)\n",
        "            dict_of_inputs['return_dict'] = True\n",
        "            dict_of_inputs['output_attentions'] = False\n",
        "            dict_of_inputs['output_hidden_states'] = False\n",
        "            dict_of_inputs['use_cache'] = True\n",
        "            with torch.no_grad():\n",
        "                outputs = model(\n",
        "                    **dict_of_inputs\n",
        "                )\n",
        "        else:\n",
        "            dict_of_inputs = {'past_key_values': past_key_values}\n",
        "            dict_of_inputs.update(model_inputs)\n",
        "            dict_of_inputs['return_dict'] = True\n",
        "            dict_of_inputs['output_attentions'] = False\n",
        "            dict_of_inputs['output_hidden_states'] = False\n",
        "            dict_of_inputs['use_cache'] = True\n",
        "            dict_of_inputs['cache_position'] = cache_position\n",
        "            with torch.no_grad():\n",
        "                outputs = model(\n",
        "                    **dict_of_inputs\n",
        "                )\n",
        "\n",
        "        if step == 0:\n",
        "            last_non_masked_idx = torch.sum(attention_mask, dim=1) - 1\n",
        "            next_token_logits = outputs.logits[\n",
        "                range(batch_size), last_non_masked_idx, :\n",
        "            ]\n",
        "        else:\n",
        "            next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "        log_prob = F.log_softmax(next_token_logits, dim=-1)\n",
        "\n",
        "        if method == \"greedy\":\n",
        "            next_tokens = greedy(next_token_logits)\n",
        "        elif method == \"sample\":\n",
        "            next_tokens = sample(next_token_logits)\n",
        "        elif method == \"temperature\":\n",
        "            next_tokens = temperature(next_token_logits, temperature=kwargs.get(\"temperature\", 0.8))\n",
        "        elif method == \"topk\":\n",
        "            next_tokens = topk(\n",
        "                next_token_logits,\n",
        "                k=kwargs.get(\"k\", 20),\n",
        "                temperature=kwargs.get(\"temperature\", 1.0),\n",
        "            )\n",
        "        elif method == \"topp\":\n",
        "            next_tokens = topp(\n",
        "                next_token_logits,\n",
        "                p=kwargs.get(\"p\", 0.7),\n",
        "                temperature=kwargs.get(\"temperature\", 1.0),\n",
        "            )\n",
        "\n",
        "        # finished sentences should have their next token be a padding token\n",
        "        next_tokens = next_tokens * unfinished_sequences + tokenizer.pad_token_id * (\n",
        "            1 - unfinished_sequences\n",
        "        )\n",
        "\n",
        "        input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
        "\n",
        "        model_kwargs = _update_model_kwargs_for_generation(\n",
        "            outputs, model_kwargs, is_encoder_decoder=model.config.is_encoder_decoder\n",
        "        )\n",
        "\n",
        "        # if eos_token was found in one sentence, set sentence to finished\n",
        "        unfinished_sequences = unfinished_sequences.mul(\n",
        "            (next_tokens != tokenizer.eos_token_id).long()\n",
        "        )\n",
        "\n",
        "        cache_position = cache_position[-1:] + 1\n",
        "\n",
        "        if unfinished_sequences.max() == 0:\n",
        "            break\n",
        "\n",
        "    response_ids = input_ids[:, input_seq_len:]\n",
        "    response_text = [\n",
        "        tokenizer.decode(\n",
        "            output, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "        )\n",
        "        for output in response_ids\n",
        "    ]\n",
        "\n",
        "    return response_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNB06qcLZ4Sr"
      },
      "outputs": [],
      "source": [
        "\"\"\"debug helper code\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # For debugging, we duplicate a single prompt 10 times so that we obtain 10 generations for the same prompt\n",
        "    dev_prompts = [dev_data[0][\"prompt\"]] * 10\n",
        "\n",
        "def print_generations(prompts, generations):\n",
        "    for prompt, generation in zip(prompts, generations):\n",
        "        print(f\"{[prompt]} ==> {[generation]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b645uIXIZ4Sr"
      },
      "source": [
        "### 1.1: Greedy Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wz_vmACyZ4Sr"
      },
      "outputs": [],
      "source": [
        "def greedy(next_token_logits):\n",
        "    \"\"\"\n",
        "    Applies greedy decoding to get the next token.\n",
        "    inputs:\n",
        "    - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "    outputs:\n",
        "    - next_tokens: Tensor(size = (B), dtype = long)\n",
        "\n",
        "    \"\"\"\n",
        "    next_tokens = None\n",
        "\n",
        "    raise NotImplementedError\n",
        "    return next_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDd8XaQmZ4Sr"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    generations = decode(dev_prompts, max_len=20, method=\"greedy\")\n",
        "    print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzJouM75Z4Sr"
      },
      "source": [
        "### 1.2: Vanilla Sampling and Temperature Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All our remaining decoding techniques involve nondeterminism-- that is, sampling. Your implementations of the remaining four decoding techniques in this assignment should all call the following function that we provide to you, `sample_next_tokens_from_probability_distribution`:"
      ],
      "metadata": {
        "id": "X7Kce-_n-rMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_next_tokens_from_probability_distribution(probs):\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "    - probs: Tensor(size = (B, V), dtype = float)\n",
        "      probs should be a tensor that contains one probability distribution per\n",
        "      instance, with each instance's probability distribution over the full\n",
        "      set of vocabulary words.\n",
        "    outputs:\n",
        "    - next_tokens: Tensor(size = (B), dtype = long)\n",
        "      next_tokens contains one token index per instance of text, corresponding\n",
        "      to the sampled next instance.\n",
        "    \"\"\"\n",
        "    next_tokens = torch.tensor([torch.multinomial(probs[i], num_samples=1) for i in range(probs.shape[0])],\n",
        "                                device=DEVICE)\n",
        "    return next_tokens"
      ],
      "metadata": {
        "id": "yPglHW9_9Y1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note that in order for your code to work with our autograder, your implementations of the remaining decoding techniques MUST each call `sample_next_tokens_from_probability_distribution`.**\n",
        "\n",
        "With that said, let's start by implementing vanilla sampling:"
      ],
      "metadata": {
        "id": "6ev80_3l_HUi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cr-DCcavZ4Sr"
      },
      "outputs": [],
      "source": [
        "def sample(next_token_logits):\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "    - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "    outputs:\n",
        "    - next_tokens: Tensor(size = (B), dtype = long)\n",
        "    \"\"\"\n",
        "\n",
        "    probs = None\n",
        "\n",
        "\n",
        "    raise NotImplementedError\n",
        "\n",
        "    # Sample from your calculated probabilities -- DON'T CHANGE THE REMAINING CODE OF THIS FUNCTION,\n",
        "    # it's important for autograding\n",
        "    next_tokens = sample_next_tokens_from_probability_distribution(probs)\n",
        "\n",
        "    return next_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbPOKqU7Z4Sr"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    set_seed()\n",
        "    generations = decode(dev_prompts, max_len=20, method=\"sample\")\n",
        "    print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2LL001LZ4Sr"
      },
      "outputs": [],
      "source": [
        "def temperature(next_token_logits, temperature):\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "    - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "    - temperature: Temperature parameter float\n",
        "    outputs:\n",
        "    - next_tokens: Tensor(size = (B), dtype = long)\n",
        "    \"\"\"\n",
        "    probs = None\n",
        "\n",
        "\n",
        "    raise NotImplementedError\n",
        "\n",
        "    # Sample from your calculated probabilities -- DON'T CHANGE THE REMAINING CODE OF THIS FUNCTION,\n",
        "    # it's important for autograding\n",
        "    next_tokens = sample_next_tokens_from_probability_distribution(probs)\n",
        "\n",
        "    return next_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smOrH8AqZ4Sr"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    set_seed()\n",
        "    generations = decode(dev_prompts, max_len=20, method=\"temperature\", temperature=0.8)\n",
        "    print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIkABBqkZ4Sr"
      },
      "source": [
        "One way to check your implementation is to see if you get same results as greedy decoding when temperature to be a very small value like 0.001 and same results as vanilla sampling when temperature = 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeCaLVC8Z4Sr"
      },
      "source": [
        "### 1.3: Top-k Sampling\n",
        "\n",
        "Useful tips:\n",
        "- Recall that in Top-k sampling, we only sample from the top-k tokens with the highest probabilities. To ensure that we set the logits other than the top-k to be -inf. You can use `float(\"-inf\")` to represent infinity in python.\n",
        "- You will find `torch.topk()` useful for getting the top-k logits and indices. Check out the [documentation](https://pytorch.org/docs/stable/generated/torch.topk.html) for the function for more details.\n",
        "- Do not forget to divide the logits by the temperature before applying softmax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cojQIXvZZ4Sr"
      },
      "outputs": [],
      "source": [
        "def topk(next_token_logits, k, temperature = 1):\n",
        "\n",
        "    \"\"\"\n",
        "    Applies the top-k sampling decoding algorithm to get the next token.\n",
        "    inputs:\n",
        "    - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "    - k: int, the number of top tokens to consider\n",
        "    - temperature: Temperature parameter float\n",
        "    outputs:\n",
        "    - next_tokens: Tensor(size = (B), dtype = long)\n",
        "    \"\"\"\n",
        "    probs = None\n",
        "\n",
        "    raise NotImplementedError\n",
        "\n",
        "    # Sample from your calculated probabilities -- DON'T CHANGE THE REMAINING CODE OF THIS FUNCTION,\n",
        "    # it's important for autograding\n",
        "    next_tokens = sample_next_tokens_from_probability_distribution(probs)\n",
        "\n",
        "    return next_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Byk7IJLZ4Sr"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    set_seed()\n",
        "    generations = decode(dev_prompts, max_len=20, method=\"topk\", k=20)\n",
        "    print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqEjepKMZ4Ss"
      },
      "source": [
        "One way to check your implementation is to see if you get same results as greedy decoding when k = 1 and same results as vanilla sampling when k = V i.e. 50257."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4IT7GiGZ4Ss"
      },
      "source": [
        "### 1.4: Top-p Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5L_KZN1fZ4Ss"
      },
      "outputs": [],
      "source": [
        "def topp(next_token_logits, p, temperature = 1):\n",
        "    \"\"\"\n",
        "    Applies the top-p sampling or nucleus sampling decoding algorithm to get the next token.\n",
        "    inputs:\n",
        "    - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "    - p: float, the cutoff probability for the top-p sampling\n",
        "    - temperature: Temperature parameter float\n",
        "    outputs:\n",
        "    - next_tokens: Tensor(size = (B), dtype = long)\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Sort the logits in descending order, and compute\n",
        "    # the cumulative probabilities `cum_probs` on the sorted logits\n",
        "    sorted_logits, sorted_indices = None, None\n",
        "    sorted_probs = None\n",
        "    cum_probs = None\n",
        "\n",
        "    raise NotImplementedError\n",
        "\n",
        "    # Create a mask to zero out all logits not in top-p\n",
        "    sorted_indices_to_remove = cum_probs > p\n",
        "    sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
        "    sorted_indices_to_remove[:, 0] = 0\n",
        "    # Restore mask to original indices\n",
        "    indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
        "\n",
        "    # indices_to_remove now contains a two-dimensional mask (instance_index by possible_next_token_index)\n",
        "    # with 0s wherever a token's probability was part of that instance's top p probability mass, and\n",
        "    # 1s wherever a token's probability was low enough for it to be knocked out of consideration for\n",
        "    # the actual sampling to follow.\n",
        "\n",
        "    probs = None\n",
        "    # TODO: populate probs with the final adjusted version of next-token probabilities (a two-dimensional\n",
        "    # tensor with one vector of next-token probabilities per instance). You'll need to use the previously calculated\n",
        "    # indices_to_remove mask.\n",
        "\n",
        "    raise NotImplementedError\n",
        "\n",
        "    # Sample from your calculated probabilities -- DON'T CHANGE THE REMAINING CODE OF THIS FUNCTION,\n",
        "    # it's important for autograding\n",
        "    next_tokens = sample_next_tokens_from_probability_distribution(probs)\n",
        "\n",
        "    return next_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ufSMaBLZ4Ss"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    set_seed()\n",
        "    generations = decode(dev_prompts, max_len=20, method=\"topp\", p=0.7)\n",
        "    print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z25KTeiiZ4Ss"
      },
      "source": [
        "To check your implementation, you can see if you get same results as greedy decoding when p = 0 and same results as vanilla sampling when p = 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f1nguMmZ4Ss"
      },
      "source": [
        "### 1.5: Evaluate!\n",
        "\n",
        "Run the following cell to obtain the evaluation results, which you should include in your writeup.\n",
        "Also don't forget to answer the questions.\n",
        "\n",
        "**Note that on a (default) CPU runtime, the following cell takes about an hour to run**, while on a T4 GPU runtime, it takes less than two minutes. **So if available, and if you haven't switched already, we'd recommend changing your runtime type to one using a GPU** (Runtime > Change runtime type, and then select any GPU-based runtime)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# <NO_AUTOGRADE> <=== Leave this code cell as is, otherwise the autograder will have trouble processing your submission"
      ],
      "metadata": {
        "id": "xjD5-QZMfxUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FltbUgYNZ4Ss"
      },
      "outputs": [],
      "source": [
        "print('Results generated on runtime with device', str(device))\n",
        "\n",
        "set_seed()\n",
        "prompts = [item[\"prompt\"] for item in test_data][:10]\n",
        "GENERATIONS_PER_PROMPT = 10\n",
        "MAX_LEN = 100\n",
        "\n",
        "for experiment in [\"greedy\", \"sample\", \"temperature\", \"topk\", \"topp\"]:\n",
        "    generations = []\n",
        "    for prompt in tqdm(prompts):\n",
        "        generations += decode(\n",
        "            [prompt] * GENERATIONS_PER_PROMPT, max_len=MAX_LEN, method=experiment\n",
        "        )\n",
        "    evaluate(generations, experiment)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Leave this code cell as is, otherwise the autograder will have trouble processing your submission\n",
        "# </NO_AUTOGRADE>"
      ],
      "metadata": {
        "id": "7rYyua7jf1Nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9a9goyBZ4Ss"
      },
      "source": [
        "You should see the following values:\n",
        "\n",
        "- For greedy:\n",
        "**perplexity** = 2.08 /\n",
        "**fluency** = 0.78 /\n",
        "**diversity** = 0.01, 0.02, 0.03\n",
        "\n",
        "- For sample:\n",
        "**perplexity** = 69.23 on CPU, 63.20 on a T4 GPU /\n",
        "**fluency** = 0.35 on CPU, 0.39 on a T4 GPU /\n",
        "**diversity** = 0.42, 0.90, 1.00 on CPU; 0.44, 0.90, 0.99 on a T4 GPU\n",
        "\n",
        "- For temperature:\n",
        "**perplexity** = 15.15 on CPU, 16.00 on a T4 GPU /\n",
        "**fluency** = 0.63 on CPU, 0.62 on a T4 GPU /\n",
        "**diversity** = 0.32, 0.78, 0.95 on CPU; 0.31, 0.79, 0.96 on a T4 GPU\n",
        "\n",
        "- For topk:\n",
        "**perplexity** = 12.53 on CPU, 13.39 on a T4 GPU /\n",
        "**fluency** = 0.71 on CPU, 0.68 on a T4 GPU /\n",
        "**diversity** = 0.27, 0.74, 0.96 on CPU; 0.25, 0.74, 0.95 on a T4 GPU\n",
        "\n",
        "- For topp:\n",
        "**perplexity** = 12.26 on CPU, 12.68 on a T4 GPU /\n",
        "**fluency** = 0.71 on CPU, 0.72 on a T4 GPU /\n",
        "**diversity** = 0.28, 0.75, 0.95 on CPU; 0.29, 0.76, 0.95 on a T4 GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwHU1Xn5Z4Ss"
      },
      "source": [
        "### *Do I always need to use all this code to generate text from a language model?*\n",
        "\n",
        "The exercises above were to help you understand the underlying mechanisms of different decoding methods. In practice, you don't need to implement all these decoding methods from scratch. You can use the `generate()` method in the ðŸ¤— Transformers library to generate text from a language model. Below we provide an example of how to use the `generate()` method to generate text from a language model. Please pay close attention to this especially if you are going to be attempting the optional section (mandatory for 517 students) on Knowledge Distillation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# <NO_AUTOGRADE> <=== Leave this code cell as is, otherwise the autograder will have trouble processing your submission"
      ],
      "metadata": {
        "id": "y88nh_cQf7vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lypquFvZ4Ss"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Step 0: Load a language model, and it's the corresponding tokenizer from the Hugging Face model hub\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", cache_dir=\"cache/\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", padding_side=\"left\", cache_dir=\"cache/\") # Setting padding_side=left is important when using these models for open ended text generation\n",
        "\n",
        "# Move the model to the device\n",
        "model = model.to(device)\n",
        "\n",
        "example_prompt = \"Once upon a time\"\n",
        "\n",
        "# Step 1: Tokenize the input prompt\n",
        "tokenized_input = tokenizer(example_prompt, return_tensors=\"pt\").to(device)\n",
        "print(\"Tokenizer output:\")\n",
        "print(tokenized_input)\n",
        "print(\"*******************\")\n",
        "\n",
        "# Step 2: Generate text from the model\n",
        "output = model.generate(**tokenized_input, max_new_tokens=100, do_sample=True, top_p=0.9, temperature=1.0,)\n",
        "print(\"Model Generate output:\")\n",
        "print(output)\n",
        "print(\"*******************\")\n",
        "\n",
        "# Step 3: Convert the output ids to text\n",
        "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"Text output:\")\n",
        "print(output_text)\n",
        "print(\"*******************\")\n",
        "# Step 3 (Optional): .generate() returns the model inputs as well. We can ignore that by slicing the output\n",
        "output_text = tokenizer.decode(output[0][len(tokenized_input.input_ids[0]):], skip_special_tokens=True)\n",
        "print(\"Text output (ignoring model inputs):\")\n",
        "print(output_text)\n",
        "print(\"*******************\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Leave this code cell as is, otherwise the autograder will have trouble processing your submission\n",
        "# </NO_AUTOGRADE>"
      ],
      "metadata": {
        "id": "rUaTrDnyf4kX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Owk9HKyZ4Ss"
      },
      "source": [
        "Let's go over each of these one by one:\n",
        "\n",
        "- **Step 1:** The `tokenizer()` function takes the input prompt and converts it into a format that the model can understand. This is essentially converting the input prompt into a sequence of tokens. Note that the tokenizer returns a dictionary with the tokenized input ids and attention mask. The `input_ids` are the token ids that the model will use as input. `\"attention_mask`\" is a mask vector that indicates if a particular token corresponds to padding. Padding is extremely important when we are dealing with variable length sequences. Through padding, we can ensure that all the sequences in a batch are of the same size. When feeding sequences with padding to a transformer based model, we need to make sure that the model doesn't attend to the padding tokens. The `attention_mask` is used the tokens that are to be ignored in the attention operation. Note that here since we only used a single input sequence, there was no need of padding and that's why all the attention mask values are 1, i.e. none of the tokens in the sequences should be ignored by attention blocks.\n",
        "\n",
        "- **Step 2:** The `generate()` function takes the tokenized input and generates a sequence of tokens as output. The function takes in a number of arguments, the most important of which are `max_new_tokens` and `do_sample`. The `max_new_tokens` argument specifies the maximum number of new tokens to be generated. The `do_sample` argument is used to turn on sampling, if false, greedy decoding is used. If `do_sample` is set to `True`, then the `top_p` and `temperature` arguments are used to control the sampling process. The `top_p` argument is the value of $p$ parameter for top-p or nucleus sampling and the `temperature` argument is used to control the temperature sampling. Notice that the `generate()` function returns as output a sequence of token ids.\n",
        "\n",
        "- **Step 3:** The `tokenizer.decode` method converts the token ids into text. The `skip_special_tokens=True` argument is used to ignore the special tokens (e.g. start of sequence, end of sequence, padding etc.) that are added by the tokenizer.\n",
        "\n",
        "- **Step 3 (Optional):** The `generate()` function returns the model inputs as well. We can ignore that by slicing the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1vFjytRZ4St"
      },
      "source": [
        "## [Optional for CSE 447]  **Section 2: Knowledge Distillation**\n",
        "\n",
        "In this part of the homework, we will learn how we can use knowledge distillation from a larger teacher model to a smaller student model. Particularly, we will be focusing on the task of text summarization and using the CNN/Daily Mail dataset. We will use Qwen2.5-1.5B-Instruct as our teacher model, which is a 1.5B parameter decoder-only mode pre-trained on 18T tokens of data and then further fine-tuned to follow instructions to perform different tasks (similar to something like ChatGPT). You can read more about Qwen2.5 models [here](https://qwenlm.github.io/blog/qwen2.5/). For the student model, we will be using the default GPT-2 model, which is a 124M parameter model.\n",
        "\n",
        "Since Qwen2.5-1.5B-Instruct is a much bigger model and trained on a lot of data, it is more capable of generating better summaries as compared to the default GPT-2 model which is a smaller model and has seen less data. Knowledge distillation is a technique to transfer the knowledge of a larger teacher model to a smaller student model. In this way, we can leverage the large amount of data and compute resources used to train the teacher model to improve the performance of the student model.\n",
        "\n",
        "This assignment will also make heavy use of the [ðŸ¤— Transformers Library](https://huggingface.co/docs/transformers/index). Don't worry if you are not familiar with the library, we will discuss its usage in detail."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# <NO_AUTOGRADE> <=== Leave this code cell as is, otherwise the autograder will have trouble processing your submission"
      ],
      "metadata": {
        "id": "cWGDbwzvf_X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGaTAzn-Z4St"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "pip install rouge-score==0.1.2\n",
        "pip install evaluate==0.4.6"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Leave this code cell as is, otherwise the autograder will have trouble processing your submission\n",
        "# </NO_AUTOGRADE>"
      ],
      "metadata": {
        "id": "aM6G7UHQgBFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PL6B_1UZ4St"
      },
      "outputs": [],
      "source": [
        "# Load packages for this section\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "if __name__ == '__main__':\n",
        "    from pprint import pprint\n",
        "    from datasets import load_from_disk, Dataset, DatasetDict\n",
        "    from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6aIoC-WZ4St"
      },
      "source": [
        "As always, we will start by loading the dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# <NO_AUTOGRADE> <=== Leave this code cell as is, otherwise the autograder will have trouble processing your submission"
      ],
      "metadata": {
        "id": "GSm0pb2egO9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snHo69hZZ4Su"
      },
      "outputs": [],
      "source": [
        "######################################################\n",
        "#  The following code is given to you. DO NOT MODIFY.\n",
        "######################################################\n",
        "parent_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
        "data_dir = os.path.join(parent_dir, \"data\")\n",
        "cnn_dm_cse447_dataset = load_from_disk(os.path.join(data_dir, \"cnn_dm_cse447_dataset.hf\"))\n",
        "cnn_dm_cse447_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v88SoF6TZ4Su"
      },
      "outputs": [],
      "source": [
        "# Preview the dataset\n",
        "print(\"Full article:\")\n",
        "pprint(cnn_dm_cse447_dataset[\"val\"][0][\"article\"])\n",
        "print(\"\\n\\n\")\n",
        "print(\"Gold summary:\")\n",
        "pprint(cnn_dm_cse447_dataset[\"val\"][0][\"summary\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Leave this code cell as is, otherwise the autograder will have trouble processing your submission\n",
        "# </NO_AUTOGRADE>"
      ],
      "metadata": {
        "id": "3UdjiGBBgTdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3wA3t0tZ4Su"
      },
      "source": [
        "### 2.1: Student Model\n",
        "\n",
        "In this exercise we will try to understand how well the student model i.e. GPT-2 does on the summarizaion task out of box. In later exercises we will try to improve the performance of the student model using knowledge distillation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4bXsjc4Z4Su"
      },
      "source": [
        "We start by loading the student model and tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQxSnqE1Z4Su"
      },
      "outputs": [],
      "source": [
        "######################################################\n",
        "#  The following code is given to you. DO NOT MODIFY.\n",
        "######################################################\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    student_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "    student_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", padding_side=\"left\")\n",
        "\n",
        "    student_model = student_model.to(device)\n",
        "    student_tokenizer.pad_token_id = student_tokenizer.eos_token_id\n",
        "    student_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcLVoEhyZ4Su"
      },
      "source": [
        "Notice GPT-2 is a decoder-only model and has 12 layers. This is the smallest model in the GPT-2 family i.e. with only 124M parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNYaLEcYZ4Su"
      },
      "source": [
        "### 2.1.1 Preparing Data for Student Model\n",
        "\n",
        "Our student model is a language model and inherently a language model's job is to predict continuations of a sequence by predicting one token at a time. To perform specific tasks like summarization using language models, we need to prepare the data in such a format such that the possible continuation of the sequence is the output we want i.e. in this case the summary of the article.\n",
        "\n",
        "The GPT-2 paper found adding a TL;DR to the end of the article helps the model in generating better summaries. Implement the `prepare_articles` function that adds a TL;DR: to the end of each input articles and then tokenizes the articles.\n",
        "\n",
        "Useful tips:\n",
        "- While tokenizing the articles, by calling `tokenizer()` make sure to set `padding=\"max_length\"` and `truncation=True` so that the articles are padded to the same length and truncated if they are longer than the maximum length. Also, make sure to set `return_tensors=\"pt\"` so that the output is a PyTorch tensor and `max_length` to the maximum length of the articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MFWvVDKZ4Su"
      },
      "outputs": [],
      "source": [
        "def prepare_articles_for_student_model(articles, student_tokenizer, max_len=1024):\n",
        "\n",
        "    \"\"\"\n",
        "    Processes and tokenizes articles into a format that can be used for summarization by the student model\n",
        "    and then tokenizes the articles using the student tokenizer.\n",
        "\n",
        "    Inputs:\n",
        "    - articles: A list of articles to be summarized.\n",
        "    - student_tokenizer: The tokenizer to use for tokenizing the articles.\n",
        "    - max_len: The maximum length of the articles.\n",
        "\n",
        "    Returns:\n",
        "    - tokenized_articles: A dictionary containing the input ids and attention mask of the tokenized articles.\n",
        "    \"\"\"\n",
        "    tokenized_articles = None\n",
        "\n",
        "\n",
        "    raise NotImplementedError\n",
        "    return tokenized_articles"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# <NO_AUTOGRADE> <=== Leave this code cell as is, otherwise the autograder will have trouble processing your submission"
      ],
      "metadata": {
        "id": "uWnVabbmgq7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYmBtSRAZ4Su"
      },
      "outputs": [],
      "source": [
        "######################################################\n",
        "#  The following code is given to you. DO NOT MODIFY.\n",
        "######################################################\n",
        "\n",
        "\n",
        "def test_prepare_articles_for_student_model():\n",
        "    # Setup\n",
        "    student_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", padding_side=\"left\")\n",
        "    student_tokenizer.pad_token_id = student_tokenizer.eos_token_id\n",
        "\n",
        "    # Test 1: Basic functionality\n",
        "    articles = [\"This is a test article.\"]\n",
        "    output = prepare_articles_for_student_model(articles, student_tokenizer)\n",
        "    assert \"input_ids\" in output, \"Output should contain input_ids\"\n",
        "    assert \"attention_mask\" in output, \"Output should contain attention_mask\"\n",
        "    assert torch.is_tensor(output[\"input_ids\"]), \"input_ids should be a tensor\"\n",
        "    assert torch.is_tensor(\n",
        "        output[\"attention_mask\"]\n",
        "    ), \"attention_mask should be a tensor\"\n",
        "\n",
        "    # Test 2: Multiple articles\n",
        "    articles = [\"First article.\", \"Second article.\", \"Third article.\"]\n",
        "    output = prepare_articles_for_student_model(articles, student_tokenizer)\n",
        "    assert (\n",
        "        output[\"input_ids\"].shape[0] == 3\n",
        "    ), \"Batch size should match number of articles\"\n",
        "    assert (\n",
        "        output[\"attention_mask\"].shape[0] == 3\n",
        "    ), \"Batch size should match number of articles\"\n",
        "\n",
        "    # Test 3: TL;DR addition\n",
        "    articles = [\"Test article.\"]\n",
        "    output = prepare_articles_for_student_model(articles, student_tokenizer)\n",
        "    decoded = student_tokenizer.decode(output[\"input_ids\"][0], skip_special_tokens=True)\n",
        "    assert \"TL;DR:\" in decoded, \"TL;DR: should be added to the article\"\n",
        "    assert decoded == \"Test article.\\nTL;DR:\", \"Article format should be correct\"\n",
        "\n",
        "    # Test 4: Padding and truncation\n",
        "    long_article = \"This is a very \" * 1000  # Create a very long article\n",
        "    short_article = \"Short.\"\n",
        "    articles = [long_article, short_article]\n",
        "    output = prepare_articles_for_student_model(articles, student_tokenizer)\n",
        "    assert (\n",
        "        output[\"input_ids\"].shape[1] == 1024\n",
        "    ), \"Should be padded/truncated to max length\"\n",
        "    assert (output[\"attention_mask\"][1] == 0).any(), \"Short article should have padding\"\n",
        "\n",
        "    print(\"All tests passed!\")\n",
        "\n",
        "test_prepare_articles_for_student_model()\n",
        "######################################################"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Leave this code cell as is, otherwise the autograder will have trouble processing your submission\n",
        "# </NO_AUTOGRADE>"
      ],
      "metadata": {
        "id": "XhBpvvAXgonk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZqZcHggZ4Su"
      },
      "source": [
        "### 2.1.2 Summarizing with Student Model\n",
        "\n",
        "Use the `generate()` method to generate summaries from the student model. Refer to the end of the Section 1 of the notebook for an example of how to use the `generate()` method. You can also learn more about the `generate()` method [here](https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationMixin.generate).\n",
        "\n",
        "Helpful tips:\n",
        "- Instead of generating summaries one by one, we recommend generating summaries in batches to speed up the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngwCQ8kJZ4Su"
      },
      "outputs": [],
      "source": [
        "def summarize_wth_student_model(\n",
        "    articles,\n",
        "    student_model,\n",
        "    student_tokenizer,\n",
        "    batch_size=8,\n",
        "    max_new_tokens=100,\n",
        "    do_sample=True,\n",
        "    p=0.9,\n",
        "    temperature=1.0,\n",
        "    device=device\n",
        "):\n",
        "\n",
        "    \"\"\"\n",
        "    Generates a list of summaries for a list of articles using the student model.\n",
        "\n",
        "    Inputs:\n",
        "    - articles: A list of articles to be summarized.\n",
        "    - student_model: The student model to use for summarization.\n",
        "    - student_tokenizer: The tokenizer corresponding to the student model.\n",
        "    - batch_size: The batch size to use for summarization.\n",
        "    - max_new_tokens: The maximum number of new tokens to generate.\n",
        "    - do_sample: Whether to use sampling or greedy decoding.\n",
        "    - p: The p parameter for top-p sampling.\n",
        "    - temperature: The temperature for sampling.\n",
        "\n",
        "    Returns:\n",
        "    - summaries: A list of summaries for the articles.\n",
        "    \"\"\"\n",
        "\n",
        "    summaries = []\n",
        "    student_model.eval()\n",
        "\n",
        "    for i in tqdm(range(0, len(articles), batch_size)):\n",
        "        # ToDo: Tokenize the batch of articles\n",
        "\n",
        "        raise NotImplementedError\n",
        "        # Move the data to the device\n",
        "        tokenized_articles = {\n",
        "            key: value.to(device) for key, value in tokenized_articles.items()\n",
        "        }\n",
        "\n",
        "        # ToDo: Generate summaries from the model using the .generate method\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "        # ToDo: Convert the generated summaries to text. Hint: Use the `batch_decode` method of the tokenizer.\n",
        "        # Make sure to only decode the generated tokens, i.e., ignore the input tokens.\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "        # Add the decoded summaries to the list\n",
        "        summaries.extend(decoded_summaries)\n",
        "\n",
        "    return summaries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# <NO_AUTOGRADE> <=== Leave this code cell as is, otherwise the autograder will have trouble processing your submission"
      ],
      "metadata": {
        "id": "rTyb6P_Vgm8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vV7PHDMUZ4Su"
      },
      "outputs": [],
      "source": [
        "def test_summarize_wth_student_model():\n",
        "    # Test 1: Basic functionality\n",
        "    articles = [\"This is a test article about AI.\"]\n",
        "    summaries = summarize_wth_student_model(\n",
        "        articles, student_model, student_tokenizer, device=device\n",
        "    )\n",
        "    assert isinstance(summaries, list), \"Output should be a list\"\n",
        "    assert len(summaries) == len(articles), \"Should generate one summary per article\"\n",
        "    assert isinstance(summaries[0], str), \"Each summary should be a string\"\n",
        "    assert len(summaries[0]) > 0, \"Summaries should not be empty\"\n",
        "\n",
        "    # Test 2: Batch processing\n",
        "    articles = [\n",
        "        \"First article.\",\n",
        "        \"Second article.\",\n",
        "        \"Third article.\",\n",
        "    ] * 4  # 12 articles\n",
        "    batch_size = 4\n",
        "    summaries = summarize_wth_student_model(\n",
        "        articles, student_model, student_tokenizer, batch_size=batch_size, device=device\n",
        "    )\n",
        "    assert len(summaries) == len(articles), \"Should generate summary for each article\"\n",
        "\n",
        "    # Test 3: Generation parameters\n",
        "    articles = [\n",
        "        \"Test article for parameter checking.\",\n",
        "        \"Test article for parameter checking.\",\n",
        "    ]\n",
        "    # Test with different generation parameters\n",
        "    summaries_greedy = summarize_wth_student_model(\n",
        "        articles, student_model, student_tokenizer, do_sample=False, device=device\n",
        "    )\n",
        "    summaries_sampling = summarize_wth_student_model(\n",
        "        articles,\n",
        "        student_model,\n",
        "        student_tokenizer,\n",
        "        do_sample=True,\n",
        "        p=0.9,\n",
        "        temperature=0.7,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    assert summaries_greedy[0] == summaries_greedy[1], \"Greedy decoding should give the same result\"\n",
        "    assert (\n",
        "        summaries_sampling[0] != summaries_sampling[1]\n",
        "    ), \"Sampling should give different results\"\n",
        "\n",
        "    # Test 4: Check only generated tokens are in summary\n",
        "    article = \"This is a test article about artificial intelligence. It contains specific phrases that should not appear in the summary unless generated.\"\n",
        "    summaries = summarize_wth_student_model(\n",
        "        [article], student_model, student_tokenizer, device=device\n",
        "    )\n",
        "    # Verify the input article text isn't in the summary\n",
        "    assert article not in summaries[0], \"Summary should not contain the input article\"\n",
        "\n",
        "    print(\"\\nAll tests passed!\")\n",
        "\n",
        "test_summarize_wth_student_model()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Leave this code cell as is, otherwise the autograder will have trouble processing your submission\n",
        "# </NO_AUTOGRADE>"
      ],
      "metadata": {
        "id": "daK1pWpjgk6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDod4dRiZ4Su"
      },
      "source": [
        "Let's generate summaries for the validation set using the student model.\n",
        "\n",
        "**If you're on a fully CPU-based runtime, this next cell will take around four hours to run.** On a T4 GPU-based runtime, it takes around seven minutes. **We'd highly recommend switching to a GPU-based runtime** (and making sure that the `device` variable, defined up near the top of this notebook, is updated) **before running this cell** (Runtime > Change runtime type)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# <NO_AUTOGRADE> <=== Leave this code cell as is, otherwise the autograder will have trouble processing your submission"
      ],
      "metadata": {
        "id": "y1gmffbvgyfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLxh3XZOZ4Su"
      },
      "outputs": [],
      "source": [
        "set_seed()\n",
        "pred_summaries = summarize_wth_student_model(\n",
        "    cnn_dm_cse447_dataset[\"val\"][\"article\"],\n",
        "    student_model,\n",
        "    student_tokenizer,\n",
        "    p=0.9,\n",
        "    temperature=1.0,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lfV2HLAZ4Sv"
      },
      "outputs": [],
      "source": [
        "# Inspect the summaries\n",
        "print(\"Article:\")\n",
        "pprint(cnn_dm_cse447_dataset[\"val\"][0][\"article\"])\n",
        "print(\"***********************\\n\\n\")\n",
        "print(\"Generated summary:\")\n",
        "pprint(pred_summaries[0])\n",
        "print(\"***********************\\n\\n\")\n",
        "print(\"Gold summary:\")\n",
        "pprint(cnn_dm_cse447_dataset[\"val\"][0][\"summary\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Leave this code cell as is, otherwise the autograder will have trouble processing your submission\n",
        "# </NO_AUTOGRADE>"
      ],
      "metadata": {
        "id": "zP0DNMqgg0mW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg5MBHgFZ4Sv"
      },
      "source": [
        "As you can see, the generated summary is not very good, with a lot of completely irrelevant text. Let's try to quantify this by evaluating the summaries using the ROUGE score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLk9Oh8cZ4Sv"
      },
      "source": [
        "### Evaluating Student Model\n",
        "\n",
        "We will use the ROUGE score to evaluate the summaries generated by the student model. ROUGE measures how similar a generated summary is to a reference (human-written) summary by comparing overlapping words and phrases. Think of it like a sophisticated \"spot the differences\" between texts.\n",
        "\n",
        "Example:\n",
        "- **Reference**: \"The cat sat on the mat\"\n",
        "- **Generated**: \"The cat lay on the mat\"\n",
        "\n",
        "ROUGE has three main variants:\n",
        "1. **ROUGE-1**: Matches single words (In example: 5/6 words match)\n",
        "2. **ROUGE-2**: Matches word pairs (In example: \"the cat\", \"on the\", \"the mat\" match)\n",
        "3. **ROUGE-L**: Finds longest matching sequences in order\n",
        "\n",
        "Scores range from 0 to 1, with higher being better. While widely used in summarization evaluation, ROUGE isn't perfect - it focuses on matching words rather than meaning.\n",
        "\n",
        "Reference: Lin, C. Y. (2004). ROUGE: A package for automatic evaluation of summaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkGvKI5_Z4Sv"
      },
      "outputs": [],
      "source": [
        "# Evaluate the summaries\n",
        "if __name__ == '__main__':\n",
        "    import evaluate\n",
        "    rouge = evaluate.load(\"rouge\")\n",
        "    scores = rouge.compute(predictions=pred_summaries, references=cnn_dm_cse447_dataset[\"val\"][\"summary\"], use_stemmer=True)\n",
        "    print(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h410-hW0Z4Sv"
      },
      "source": [
        "You should see the following scores:\n",
        "- rouge1: 0.19\n",
        "- rouge2: 0.03\n",
        "- rougeL: 0.13\n",
        "- rougeLsum: 0.16\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZQzGUf6Z4Sv"
      },
      "source": [
        "The model performs relatively poorly on the dataset, especially considering the ROUGE-2 and ROUGE-L scores, which are very low. Let's see if we can improve this by using a teacher model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwfUKie5Z4Sv"
      },
      "source": [
        "### 2.2 Generate Data from a Teacher Model\n",
        "\n",
        "We will now start with step 1 of knowledge distillation i.e. generating data from a teacher model. As we mentioned earlier, we will use the Qwen2.5-1.5B-Instruct model as our teacher model. Note that Qwen2.5-1.5B-Instruct is an instruction tuned model, which is different from a base language model like GPT-2. Instruction tuned models are obtained from base language models by fine-tuning them on a diverse set of instructions and their desired outputs. This enables the model to become better at following instructions and is a big factor in the success of recent large language models."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# <NO_AUTOGRADE> <=== Leave this code cell as is, otherwise the autograder will have trouble processing your submission"
      ],
      "metadata": {
        "id": "QNPSTX1rhCfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9DulWKeZ4Sv"
      },
      "outputs": [],
      "source": [
        "# Load a teacher model.\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "teacher_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-1.5B-Instruct\", cache_dir=\"cache/\"\n",
        ")\n",
        "teacher_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-1.5B-Instruct\", cache_dir=\"cache/\", padding_side=\"left\"\n",
        ")\n",
        "teacher_model = teacher_model.to(device)\n",
        "teacher_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Leave this code cell as is, otherwise the autograder will have trouble processing your submission\n",
        "# </NO_AUTOGRADE>"
      ],
      "metadata": {
        "id": "uKX_6x4JhENu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dasMFq1yZ4Sv"
      },
      "source": [
        "### 2.2.1 Preparing Data for Teacher Model\n",
        "\n",
        "We need to prepare the data for the teacher model in a specific format. Since the teacher model is an instruction-tuned model, it has been adapted to follow instructions. Hence, instead of formatting the text according to a completion problem, we need to format it according to an instruction following the problem. We will do this by adding an instruction to the beginning of each article, i.e., \"Summarize the following article.\" Further, we will also instruct the model to output the summary in a specific format by appending a suffix to the end of each article, i.e., \"Start your summary with 'TL;DR:'\" In short, the articles should be formatted as follows:\n",
        "\n",
        "`\"Summarize the following article: <article>. Start your summary with 'TL;DR:'\"`\n",
        "\n",
        "**To match our implementation, you need to *exactly* match the above format.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcc4B0S5Z4Sv"
      },
      "outputs": [],
      "source": [
        "def prepare_articles_teacher(articles, teacher_tokenizer, max_len=1024):\n",
        "\n",
        "    \"\"\"\n",
        "    Prepares the articles for the teacher model and performs tokenization.\n",
        "\n",
        "    Inputs\n",
        "    \"\"\"\n",
        "\n",
        "    # ToDo: Add the instructions as specified above to each article\n",
        "    articles_with_instructions = None\n",
        "\n",
        "\n",
        "    raise NotImplementedError\n",
        "\n",
        "    # The instruction-tuned models to be in chat format. Read more here: https://huggingface.co/docs/transformers/main/en/chat_templating\n",
        "    articles_chat = [[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant and an expert at summarizing articles.\"},\n",
        "        {\"role\": \"user\", \"content\": article}\n",
        "    ] for article in articles_with_instructions]\n",
        "    # Apply chat template to the articles\n",
        "    articles_chat = teacher_tokenizer.apply_chat_template(articles_chat, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # ToDo: Tokenize the articles. Hint: Set `add_special_tokens=False` since special tokens are already added in the chat template.\n",
        "\n",
        "\n",
        "    raise NotImplementedError\n",
        "\n",
        "    return tokenized_articles"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# <NO_AUTOGRADE> <=== Leave this code cell as is, otherwise the autograder will have trouble processing your submission"
      ],
      "metadata": {
        "id": "HskDvhvShOsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1BqeSk0Z4Sv"
      },
      "outputs": [],
      "source": [
        "def test_prepare_articles_teacher():\n",
        "\n",
        "    # Test 1: Basic functionality\n",
        "    articles = [\"This is a test article.\"]\n",
        "    output = prepare_articles_teacher(articles, teacher_tokenizer)\n",
        "\n",
        "    assert \"input_ids\" in output, \"Output should contain input_ids\"\n",
        "    assert \"attention_mask\" in output, \"Output should contain attention_mask\"\n",
        "    assert torch.is_tensor(output[\"input_ids\"]), \"input_ids should be a tensor\"\n",
        "    assert torch.is_tensor(\n",
        "        output[\"attention_mask\"]\n",
        "    ), \"attention_mask should be a tensor\"\n",
        "\n",
        "    # Test 2: Instruction addition\n",
        "    decoded = teacher_tokenizer.decode(output[\"input_ids\"][0], skip_special_tokens=True)\n",
        "    assert (\n",
        "        decoded == f\"system\\nYou are a helpful assistant and an expert at summarizing articles.\\nuser\\nSummarize the following article: {articles[0]}. Start your summary with 'TL;DR:'\\nassistant\\n\"\n",
        "    ), \"Should EXACTLY follow given summarization instruction formatting\"\n",
        "\n",
        "    # Test 3: Multiple articles\n",
        "    articles = [\"First article.\", \"Second article.\", \"Third article.\"]\n",
        "    output = prepare_articles_teacher(articles, teacher_tokenizer)\n",
        "    assert (\n",
        "        output[\"input_ids\"].shape[0] == 3\n",
        "    ), \"Batch size should match number of articles\"\n",
        "    assert (\n",
        "        output[\"attention_mask\"].shape[0] == 3\n",
        "    ), \"Batch size should match number of articles\"\n",
        "\n",
        "    # Test 4: Padding and truncation\n",
        "    long_article = \"This is a very \" * 1000  # Create a very long article\n",
        "    short_article = \"Short.\"\n",
        "    articles = [long_article, short_article]\n",
        "    output = prepare_articles_teacher(articles, teacher_tokenizer, max_len=1024)\n",
        "    assert (\n",
        "        output[\"input_ids\"].shape[1] == 1024\n",
        "    ), \"Should be padded/truncated to max length\"\n",
        "    assert (output[\"attention_mask\"][1] == 0).any(), \"Short article should have padding\"\n",
        "\n",
        "    print(\"All tests passed!\")\n",
        "\n",
        "test_prepare_articles_teacher()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Leave this code cell as is, otherwise the autograder will have trouble processing your submission\n",
        "# </NO_AUTOGRADE>"
      ],
      "metadata": {
        "id": "ltBffYdxhIiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfEleszDZ4Sv"
      },
      "source": [
        "Let's have a look at the tokenized articles."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# <NO_AUTOGRADE> <=== Leave this code cell as is, otherwise the autograder will have trouble processing your submission"
      ],
      "metadata": {
        "id": "Nt8Dv771hLz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6XIj37JZ4Sv"
      },
      "outputs": [],
      "source": [
        "tokenized_articles = prepare_articles_teacher(\n",
        "    cnn_dm_cse447_dataset[\"val\"][\"article\"][:2], teacher_tokenizer\n",
        ")\n",
        "decoded_articles = teacher_tokenizer.batch_decode(\n",
        "    tokenized_articles[\"input_ids\"], skip_special_tokens=True\n",
        ")\n",
        "pprint(decoded_articles[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Leave this code cell as is, otherwise the autograder will have trouble processing your submission\n",
        "# </NO_AUTOGRADE>"
      ],
      "metadata": {
        "id": "uOsIc3ylhLDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaUYQEqyZ4Sv"
      },
      "source": [
        "Notice the system, user, and assistant tags in the prompt. The system tag is followed by an instruction to ground the model to be a helpful assistant and be an expert at summarizing articles. The user tag is followed by the article to be summarized and the instruction about the task. We have the assistant tag in the end and the model is expected to generate the summary after the assistant tag."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvSWKwaJZ4Sv"
      },
      "source": [
        "### 2.2.2. Generate summaries with teacher model\n",
        "\n",
        "Similar to the student model, we will use the `generate()` method to generate summaries from the teacher model. Implement the `summarize_with_teacher_model` function below to do this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVKhoUK3Z4Sv"
      },
      "outputs": [],
      "source": [
        "def summarize_with_teacher_model(\n",
        "    articles,\n",
        "    teacher_model,\n",
        "    teacher_tokenizer,\n",
        "    batch_size=8,\n",
        "    max_new_tokens=100,\n",
        "    do_sample=True,\n",
        "    p=0.9,\n",
        "    temperature=1.0,\n",
        "    device=device,\n",
        "):\n",
        "\n",
        "    \"\"\"\n",
        "    Generates summaries for a list of articles using the teacher model.\n",
        "    Essentially the same as the function `summarize_wth_student_model`, but we will use the prepare articles function for the teacher model here.\n",
        "    \"\"\"\n",
        "    summaries = []\n",
        "    teacher_model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(articles), batch_size)):\n",
        "\n",
        "            raise NotImplementedError\n",
        "            # Add the decoded summaries to the list\n",
        "            summaries.extend(decoded_summaries)\n",
        "\n",
        "    return summaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5txO_vwZ4Sv"
      },
      "source": [
        "Let's first check how well the teacher model performs on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# <NO_AUTOGRADE> <=== Leave this code cell as is, otherwise the autograder will have trouble processing your submission"
      ],
      "metadata": {
        "id": "ZXqNh7HxhRKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OY8M8NAZ4Sv"
      },
      "outputs": [],
      "source": [
        "set_seed()\n",
        "# Since the teacher model is large, we will only evaluate on a small subset of the validation set.\n",
        "val_summaries_generated = summarize_with_teacher_model(\n",
        "    cnn_dm_cse447_dataset[\"val\"][\"article\"][:100],\n",
        "    teacher_model,\n",
        "    teacher_tokenizer,\n",
        "    batch_size=8,\n",
        "    max_new_tokens=100,\n",
        "    do_sample=True,\n",
        "    p=0.9,\n",
        "    temperature=1.0,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UdBMXodZ4Sw"
      },
      "outputs": [],
      "source": [
        "# Inspect the summaries\n",
        "print(\"Article:\")\n",
        "pprint(cnn_dm_cse447_dataset[\"val\"][0][\"article\"])\n",
        "print(\"***********************\\n\\n\")\n",
        "print(\"Generated summary:\")\n",
        "pprint(val_summaries_generated[0])\n",
        "print(\"***********************\\n\\n\")\n",
        "print(\"Gold summary:\")\n",
        "pprint(cnn_dm_cse447_dataset[\"val\"][0][\"summary\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJmIviM1Z4Sw"
      },
      "source": [
        "You should see that the generated summary is much better than the one generated by the student model. Let's now evaluate the summaries generated by the teacher model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFAZFK4BZ4Sw"
      },
      "outputs": [],
      "source": [
        "set_seed()\n",
        "teacher_rouge = evaluate.load(\"rouge\")\n",
        "scores = teacher_rouge.compute(\n",
        "    predictions=val_summaries_generated,\n",
        "    references=cnn_dm_cse447_dataset[\"val\"][\"summary\"][:100],\n",
        "    use_stemmer=True\n",
        ")\n",
        "print(scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Leave this code cell as is, otherwise the autograder will have trouble processing your submission\n",
        "# </NO_AUTOGRADE>"
      ],
      "metadata": {
        "id": "O8C5F90_hX1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UqP40oyZ4Sw"
      },
      "source": [
        "You should see the following scores:\n",
        "- rouge1: 0.262\n",
        "- rouge2: 0.082\n",
        "- rougeL: 0.182\n",
        "- rougeLsum: 0.211\n",
        "\n",
        "While these are only on a small subset of the validation set, we also ran the teacher model on the entire validation set and got the following scores:\n",
        "- rouge1: 0.284\n",
        "- rouge2: 0.085\n",
        "- rougeL: 0.188\n",
        "- rougeLsum: 0.229"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka-vN7i6Z4Sw"
      },
      "source": [
        "As you can see, the teacher model performs much better than the student model. While the numbers are not super high, note that our teacher model is only 1.5B parameters, which is considered a tiny model in the current LLM landscape. Bigger models with 8B, 30B, 70B, or even higher are expected to perform even better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dePxpZAkZ4Sw"
      },
      "source": [
        "#### Generating Data for Distillation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiUlqHfmZ4Sw"
      },
      "source": [
        "We can now generate data for distillation. We will use the articles from the training dataset and generate summaries for those articles using the teacher model. Note that we do have the original summaries for the training set, but we are not using them for distillation. Our goal is to show you how distillation works and how you can use it for your own use cases where you might not have labelled training data available.\n",
        "\n",
        "**Note that in our case, to save you time on this assignment, we've actually pre-generated this data;** you *can* run the next two blocks of code to re-generate the data, but they're mostly here so that you know exactly how the `kd_dataset.hf` data we provide you with was generated (and how you'd generate your own data for distillation in another setting). **For this assignment, though, you can skip ahead a couple of code blocks to the one defining `kd_dataset`.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# <NO_AUTOGRADE> <=== Leave this code cell as is, otherwise the autograder will have trouble processing your submission"
      ],
      "metadata": {
        "id": "DvBn7r3qhc73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S-BeJxuZ4Sw"
      },
      "outputs": [],
      "source": [
        "# Again, for this assignment, you can skip running this code block (and the one right after it)\n",
        "# and instead run the one that just contains:\n",
        "#\n",
        "# kd_dataset = load_from_disk(f\"{data_dir}/kd_dataset.hf\")\n",
        "#\n",
        "# These two code blocks are only here to show you how the data for distillation was generated.\n",
        "set_seed()\n",
        "train_summaries_generated = summarize_with_teacher_model(\n",
        "    cnn_dm_cse447_dataset[\"train\"][\"article\"][:1000],\n",
        "    teacher_model,\n",
        "    teacher_tokenizer,\n",
        "    batch_size=8,\n",
        "    max_new_tokens=100,\n",
        "    do_sample=True,\n",
        "    p=0.9,\n",
        "    temperature=1.0,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pywa-Ra3Z4Sw"
      },
      "outputs": [],
      "source": [
        "if not os.path.isfile(f\"{data_dir}/kd_dataset.hf/data-00000-of-00001.arrow\"):\n",
        "    kd_dataset = {\n",
        "        \"article\": cnn_dm_cse447_dataset[\"train\"][\"article\"][:1000][:1000],\n",
        "        \"summary\": train_summaries_generated,\n",
        "        \"gold_summary\": cnn_dm_cse447_dataset[\"train\"][\"summary\"][:1000][:1000]\n",
        "    }\n",
        "    kd_dataset = Dataset.from_dict(kd_dataset)\n",
        "    kd_dataset\n",
        "\n",
        "    # Save the dataset to disk, so that you can load it later without re-generating the data.\n",
        "    kd_dataset.save_to_disk(f\"{data_dir}/kd_dataset.hf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz9nXpt_Z4Sw"
      },
      "source": [
        "For this assignment, you can directly load our previously teacher-model-generated data using the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kd_dataset = load_from_disk(f\"{data_dir}/kd_dataset.hf\")"
      ],
      "metadata": {
        "id": "aPCQUQ6X8Pzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Leave this code cell as is, otherwise the autograder will have trouble processing your submission\n",
        "# </NO_AUTOGRADE>"
      ],
      "metadata": {
        "id": "7r9hleiShfA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE9_Z6UtZ4Sw"
      },
      "source": [
        "### 3. Fine-tuning student model with teacher model's generated data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qsym2NPfZ4Sw"
      },
      "source": [
        "#### 3.1 Preparing data for distillation\n",
        "\n",
        "You will now implement the `prepare_data_for_distillation` function to prepare the data for distillation. This function will format the data in a specific way so that it can be used to fine-tune the student model. You will follow pretty much the same process as you did for the student model in `prepare_articles_for_student_model` with a few changes.\n",
        "\n",
        "- First we will include the summaries in the input text along with the articles. This is done because we are now training the student model to generate summaries from the articles. Hence the format of the input text will be `<article>\\nTL;DR:<summary>`.\n",
        "- In the tokenization dictionary, we now need to add a new key, `labels,` which contains the labels to train the language model. For language models, the labels are the same as the input IDs since the model is expected to generate the next word in the sequence. However, while fine-tuning, we want the model to learn how to generate the summaries from the articles and we do not care about the model learning to predict tokens in the original articles. Therefore, we replace the labels for the prompt tokens with -100, which is a special token id that is used to signal the loss function to ignore the loss for those tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fzPxUq_Z4Sw"
      },
      "outputs": [],
      "source": [
        "def prepare_data_for_distillation(article, summary, student_tokenizer, max_length=1024):\n",
        "\n",
        "    \"\"\"\n",
        "    Prepares the data for distillation.\n",
        "\n",
        "    Inputs:`\n",
        "    - article: The article to be summarized.\n",
        "    - summary: Summary to be used as labels for distillation.\n",
        "    - student_tokenizer: Tokenizer for the student model.\n",
        "    - max_length: Maximum length of the tokenized input.\n",
        "\n",
        "    Returns:\n",
        "    - tokenized_data: Tokenized data in the format of a dictionary with keys `input_ids`, `attention_mask`, and `labels`.\n",
        "\n",
        "    Note: Unlike the functions preparing data for student and techer models before, this function only takes a single\n",
        "    article as input instead of a list of articles. We are doing this because while training we want to create batches\n",
        "    dynamically such that they are padded based on the longest article in the batch. Earlier functions were padding all\n",
        "    the articles with the same length. While it doesn't make any difference in the performance whether you use\n",
        "    dynamic padding or fixed padding, dynamic padding can be much more compute and memory efficient, which is especially\n",
        "    important for training\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Format the article and summary for distillation.\n",
        "    # Hint: `summary` might already contain the TL;DR: prefix, make sure you are not adding it twice!\n",
        "    article_wth_summary = None\n",
        "    article_prompt = None  # This should only contain the article and <article>\\nTL;DR: you will need this to set the labels for the prompt tokens to -100\n",
        "\n",
        "\n",
        "    raise NotImplementedError\n",
        "\n",
        "    # TODO: Tokenize `articles_wth_summaries`. Note set padding=False, since we want to do dyanmic padding during training.\n",
        "    tokenized_data = None\n",
        "    tokenized_prompt = None\n",
        "\n",
        "\n",
        "    raise NotImplementedError\n",
        "\n",
        "    tokenized_data[\"input_ids\"], tokenized_data[\"attention_mask\"] = tokenized_data[\n",
        "        \"input_ids\"\n",
        "    ][0], tokenized_data[\"attention_mask\"][0]\n",
        "    tokenized_prompt[\"input_ids\"], tokenized_prompt[\"attention_mask\"] = (\n",
        "        tokenized_prompt[\"input_ids\"][0],\n",
        "        tokenized_prompt[\"attention_mask\"][0],\n",
        "    )\n",
        "\n",
        "    # Set the labels to the input IDs\n",
        "    tokenized_data[\"labels\"] = tokenized_data[\"input_ids\"].clone()\n",
        "\n",
        "    # TODO: Replace the labels for the prompt tokens with -100.\n",
        "\n",
        "\n",
        "    raise NotImplementedError\n",
        "\n",
        "    return tokenized_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# <NO_AUTOGRADE> <=== Leave this code cell as is, otherwise the autograder will have trouble processing your submission"
      ],
      "metadata": {
        "id": "6i9_e2tBhjky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLr6moS-Z4Sw"
      },
      "outputs": [],
      "source": [
        "def test_prepare_data_for_distillation():\n",
        "    # Setup\n",
        "    student_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", padding_side=\"right\")\n",
        "    student_tokenizer.pad_token_id = student_tokenizer.eos_token_id\n",
        "\n",
        "    # Test 1: Basic functionality\n",
        "    article = \"This is a test article.\"\n",
        "    summary = \"This is a test summary.\"\n",
        "    output = prepare_data_for_distillation(article, summary, student_tokenizer)\n",
        "    assert \"input_ids\" in output, \"Output should contain input_ids\"\n",
        "    assert \"attention_mask\" in output, \"Output should contain attention_mask\"\n",
        "    assert \"labels\" in output, \"Output should contain labels\"\n",
        "    assert torch.is_tensor(output[\"input_ids\"]), \"input_ids should be a tensor\"\n",
        "    assert torch.is_tensor(\n",
        "        output[\"attention_mask\"]\n",
        "    ), \"attention_mask should be a tensor\"\n",
        "    assert torch.is_tensor(output[\"labels\"]), \"labels should be a tensor\"\n",
        "\n",
        "    # Test 2: Don't re-copy TL;DR from summary if it's there\n",
        "    article = \"This is a test article.\"\n",
        "    summary = \"TL;DR:This is a test summary.\"\n",
        "    output = prepare_data_for_distillation(article, summary, student_tokenizer)\n",
        "    decoded = student_tokenizer.decode(output[\"input_ids\"], skip_special_tokens=True)\n",
        "    assert decoded == \"This is a test article.\\nTL;DR:This is a test summary.\", \\\n",
        "        \"Doesn't handle duplicated TL;DR in a way that matches our implementation\" + '\\n' + decoded\n",
        "    summary = \"TL;DR: This is a test summary.\"\n",
        "    output = prepare_data_for_distillation(article, summary, student_tokenizer)\n",
        "    decoded = student_tokenizer.decode(output[\"input_ids\"], skip_special_tokens=True)\n",
        "    assert decoded == \"This is a test article.\\nTL;DR:This is a test summary.\", \\\n",
        "        \"Doesn't handle duplicated TL;DR in a way that matches our implementation\"\n",
        "\n",
        "    # Test 3: Correct label masking\n",
        "    article = \"This is a test article.\"\n",
        "    summary = \"This is a test summary.\"\n",
        "    output = prepare_data_for_distillation(article, summary, student_tokenizer)\n",
        "    input_ids = output[\"input_ids\"]\n",
        "    labels = output[\"labels\"]\n",
        "\n",
        "    # Ensure labels for the article part are -100\n",
        "    article_length = len(student_tokenizer(article + \"\\nTL;DR:\")[\"input_ids\"])\n",
        "    assert all(\n",
        "        label == -100 for label in labels[:article_length]\n",
        "    ), \"Labels for article tokens should be -100\"\n",
        "\n",
        "    # Ensure labels for the summary part are not -100\n",
        "    assert all(\n",
        "        label != -100 for label in labels[article_length:]\n",
        "    ), \"Labels for summary tokens should not be -100\"\n",
        "\n",
        "    # Test 4: Padding and truncation\n",
        "    long_article = \"This is a very \" * 1000  # Create a very long article\n",
        "    long_summary = \"Summary \" * 1000\n",
        "    short_article = \"Short.\"\n",
        "    short_summary = \"Short.\"\n",
        "    output = prepare_data_for_distillation(\n",
        "        long_article, long_summary, student_tokenizer, max_length=1024\n",
        "    )\n",
        "    assert (\n",
        "        output[\"input_ids\"].shape[0] == 1024\n",
        "    ), \"Long inputs should be truncated to max length\"\n",
        "\n",
        "    output = prepare_data_for_distillation(\n",
        "        short_article, short_summary, student_tokenizer, max_length=1024\n",
        "    )\n",
        "    assert (output[\"attention_mask\"][1] != 0).all(), \"Short article should no have padding\"\n",
        "    print(\"All tests passed!\")\n",
        "\n",
        "test_prepare_data_for_distillation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "948UJHs7Z4Sw"
      },
      "source": [
        "Let's now prepare the data for distillation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSp3eaN6lyZm"
      },
      "outputs": [],
      "source": [
        "kd_dataset = load_from_disk(f\"{data_dir}/kd_dataset.hf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y83qgXB2Z4Sw"
      },
      "outputs": [],
      "source": [
        "train_tokenized_data = kd_dataset.map(\n",
        "    lambda example: prepare_data_for_distillation(\n",
        "        example[\"article\"],\n",
        "        example[\"summary\"],\n",
        "        student_tokenizer,\n",
        "        max_length=1024,\n",
        "    ),\n",
        "    batched=False,\n",
        "    remove_columns=kd_dataset.column_names,\n",
        ")\n",
        "train_tokenized_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14i7bAwaj-rG"
      },
      "outputs": [],
      "source": [
        "val_tokenized_data = cnn_dm_cse447_dataset[\"val\"].map(\n",
        "    lambda example: prepare_data_for_distillation(\n",
        "        example[\"article\"],\n",
        "        example[\"summary\"],\n",
        "        student_tokenizer,\n",
        "        max_length=1024,\n",
        "    ),\n",
        "    batched=False,\n",
        "    remove_columns=cnn_dm_cse447_dataset[\"val\"].column_names,\n",
        ")\n",
        "val_tokenized_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbExZOLlZ4Sx"
      },
      "outputs": [],
      "source": [
        "# Let's have a look at the tokenized data\n",
        "tokenized_data = train_tokenized_data[0]\n",
        "print(\"Input IDs:\")\n",
        "print(tokenized_data[\"input_ids\"])\n",
        "print(\"Labels:\")\n",
        "print(tokenized_data[\"labels\"])\n",
        "print(\"***********************\\n\\n\")\n",
        "\n",
        "print(\"Input Text:\")\n",
        "pprint(student_tokenizer.decode(tokenized_data[\"input_ids\"], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh9Q8mnNZ4Sx"
      },
      "source": [
        "### Fine-tuning the student model\n",
        "\n",
        "We are now ready to fine-tune the student model. While this might sound like a daunting task, we actually need not write much code for this. Transformers library has a `Trainer` class that takes care of the fine-tuning process. We will use the `TrainingArguments` class to set the training arguments and the `Trainer` class to fine-tune the model. Below is the code to fine-tune the student model.\n",
        "\n",
        "**If you find yourself getting an OutOfMemory error, try changing `per_device_train_batch_size` and `per_device_eval_batch_size` to 2, and upping `gradient_accumulation_steps` to 8.** While this might move your subsequent results around a bit from the ones we report due to producing a slightly different sequence of random seeded numbers, conceptually this change will result in the same model, just trained a bit more slowly.\n",
        "\n",
        "You're welcome to change training arguments if you like (except for `report_to`), but regardless of whether you do or not, **leave the output that's generated when the following cell is run as-is; we check it to see whether finetuning successfully completed.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLltA7hnZ4Sx"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
        "\n",
        "if device.lower().startswith('cuda'):\n",
        "    try:\n",
        "        # because otherwise, if we're on a T4 GPU runtime,\n",
        "        # holding cola_model on the GPU too will lead to OOMemory errors in the code block below\n",
        "        del cola_model\n",
        "    except NameError as e:\n",
        "        pass\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "set_seed()\n",
        "student_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"checkpoints/\",\n",
        "    logging_steps=1,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=27,\n",
        "    logging_first_step=True,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=3,  # don't decrease this, we check that at least three epochs completed\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        "    learning_rate=2e-5,\n",
        "    warmup_steps=100,\n",
        "    seed=42,\n",
        "    report_to=\"tensorboard\",  # LEAVE THIS LINE THE SAME, the autograder needs it\n",
        "    gradient_checkpointing=True\n",
        ")\n",
        "\n",
        "# LEAVE ALL REMAINING CODE IN THIS CODE BLOCK THE SAME, the autograder needs it\n",
        "\n",
        "# Create a data collator for dynamic padding\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=student_tokenizer, model=student_model, padding=\"longest\"\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=student_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized_data,\n",
        "    eval_dataset=val_tokenized_data,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "print('Training complete.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj7QYKZ8Z4Sx"
      },
      "source": [
        "At the end of training you should see a training loss of around 2.44 and validation loss of 3.00 (despite calling `set_seed()`, the exact numbers you get might shift a bit on different training runs, though. So don't be worried if your numbers are a little off from ours)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJVAd5GlZ4Sx"
      },
      "source": [
        "Let's now generate summaries with the fine-tuned student model and evaluate the performance.\n",
        "\n",
        "**Leave the code in the following code block, AND the output that's generated once you run it, as-is; the autograder needs both.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVDjSlYfZ4Sx"
      },
      "outputs": [],
      "source": [
        "# Leave this code cell as is, otherwise the autograder will have trouble processing your submission\n",
        "set_seed()\n",
        "# Padding side left for generation\n",
        "student_tokenizer.padding_side = \"left\"\n",
        "pred_summaries_ft_student = summarize_wth_student_model(\n",
        "    cnn_dm_cse447_dataset[\"val\"][\"article\"], student_model, student_tokenizer, p=0.9, temperature=1.0, device=device\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eMd30pWZ4Sx"
      },
      "outputs": [],
      "source": [
        "# Inspect the summaries\n",
        "print(\"Article:\")\n",
        "pprint(cnn_dm_cse447_dataset[\"val\"][0][\"article\"])\n",
        "print(\"***********************\\n\\n\")\n",
        "print(\"Generated summary:\")\n",
        "pprint(pred_summaries_ft_student[0])\n",
        "print(\"***********************\\n\\n\")\n",
        "print(\"Gold summary:\")\n",
        "pprint(cnn_dm_cse447_dataset[\"val\"][0][\"summary\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_qNIFl2Z4Sx"
      },
      "source": [
        "While not perfect, the summaries generated by the fine-tuned student model are much better than the ones generated by the student model before fine-tuning. Also, note that we only used a very small number of examples i.e. 1000 to distill the model, in practice it is common to use a larger dataset. Let's now evaluate the performance of the fine-tuned student model.\n",
        "\n",
        "**Leave the code in the following code block, AND the output that's generated once you run it, as-is; the autograder needs both.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qInWWVF4Z4Sx"
      },
      "outputs": [],
      "source": [
        "# Leave this code cell as is, otherwise the autograder will have trouble processing your submission\n",
        "import evaluate\n",
        "teacher_rouge = evaluate.load(\"rouge\")\n",
        "scores = teacher_rouge.compute(\n",
        "    predictions=pred_summaries_ft_student, references=cnn_dm_cse447_dataset[\"val\"][\"summary\"], use_stemmer=True\n",
        ")\n",
        "print(scores)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Leave this code cell as is, otherwise the autograder will have trouble processing your submission\n",
        "# </NO_AUTOGRADE>"
      ],
      "metadata": {
        "id": "4Tw08bWahsC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmc8FiL0Z4Sx"
      },
      "source": [
        "You should see the following scores:\n",
        "- rouge1: 0.230\n",
        "- rouge2: 0.057\n",
        "- rougeL: 0.145\n",
        "- rougeLsum: 0.183\n",
        "\n",
        "(Once again, though, you might observe a small deviation from our numbers in this section, even if you did everything correctly, despite us calling `set_seed()` earlier. Don't worry if you do see this small deviation. This kind of slight nondeterminism despite best efforts is a common issue when training on GPUs, unfortunately...)\n",
        "\n",
        "As you can see, the fine-tuned student model performs better than the student model before fine-tuning. This is impressive because we used very small teacher and student models and only used 1000 examples for distillation.\n",
        "\n",
        "In practice, you would want to use a much bigger teacher model and a bigger student model to get better performance. The purpose of this exercise is to give you an idea of how distillation works and how you can use it for your own use cases."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}